{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem Set 1: Text Classification\n",
    "=============\n",
    "\n",
    "In this problem set, you will build a system for automatically classifying song lyrics comments by era. You will:\n",
    "\n",
    "- Do some basic text processing, tokenizing your input and converting it into a bag-of-words representation\n",
    "- Build a machine learning classifier based on the generative model, using Naive Bayes\n",
    "- Evaluate your classifiers and examine what they have learned\n",
    "- Build a machine learning classifier based on the discriminative model, using Perceptron\n",
    "- Build a logistic regression classifier using PyTorch\n",
    "- Implement techniques to improve your classifier, and compete on Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setup\n",
    "\n",
    "In order to develop this assignment, you will need [python 3.6](https://www.python.org/downloads/) and the following libraries. Most if not all of these are part of [anaconda](https://www.continuum.io/downloads), so a good starting point would be to install that.\n",
    "\n",
    "- [jupyter](http://jupyter.readthedocs.org/en/latest/install.html)\n",
    "- numpy (This will come if you install scipy like above, but if not install separately)\n",
    "- [matplotlib](http://matplotlib.org/users/installing.html)\n",
    "- [nosetests](https://nose.readthedocs.org/en/latest/)\n",
    "- [pandas](http://pandas.pydata.org/) Dataframes\n",
    "\n",
    "Here is some help on installing packages in python: https://packaging.python.org/installing/. You can use ```pip --user``` to install locally without sudo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this assignment\n",
    "\n",
    "- This is a Jupyter notebook. You can execute cell blocks by pressing control-enter.\n",
    "- Most of your coding will be in the python source files in the directory ```gtnlplib```.\n",
    "- The directory ```tests``` contains unit tests that will be used to grade your assignment, using ```nosetests```. You should run them as you work on the assignment to see that you're on the right track. You are free to look at their source code, if that helps -- though most of the relevant code is also here in this notebook. Learn more about running unit tests at http://pythontesting.net/framework/nose/nose-introduction/\n",
    "- You may want to add more tests, but that is completely optional. \n",
    "- **To submit this assignment, run the script ```make-submission.sh```, and submit the tarball ```pset1-submission.tgz``` on Canvas.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Python version\n",
      "python: 3.6.4 (default, Dec 19 2017, 15:24:51) \n",
      "[GCC 4.2.1 Compatible Apple LLVM 9.0.0 (clang-900.0.39.2)]\n"
     ]
    }
   ],
   "source": [
    "print('My Python version')\n",
    "\n",
    "print('python: {}'.format(sys.version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nose\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My library versions\n",
      "pandas: 0.22.0\n",
      "numpy: 1.14.0\n",
      "scipy: 1.0.0\n",
      "matplotlib: 2.1.2\n",
      "nose: 1.3.7\n",
      "torch: 0.3.0.post4\n"
     ]
    }
   ],
   "source": [
    "print('My library versions')\n",
    "\n",
    "print('pandas: {}'.format(pd.__version__))\n",
    "print('numpy: {}'.format(np.__version__))\n",
    "print('scipy: {}'.format(sp.__version__))\n",
    "print('matplotlib: {}'.format(matplotlib.__version__))\n",
    "print('nose: {}'.format(nose.__version__))\n",
    "print('torch: {}'.format(torch.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test whether your libraries are the right version, run:\n",
    "\n",
    "`nosetests tests/test_environment.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\r\n",
      "----------------------------------------------------------------------\r\n",
      "Ran 1 test in 0.001s\r\n",
      "\r\n",
      "OK\r\n"
     ]
    }
   ],
   "source": [
    "# use ! to run shell commands in notebook\n",
    "! nosetests tests/test_environment.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocessing\n",
    "\n",
    "Total: 1 point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('lyrics-train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dataframe is a structured representation of your data. You can preview a dataframe using `head()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Era</th>\n",
       "      <th>Lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pre-1980</td>\n",
       "      <td>come on come on let me show you where its at a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1980s</td>\n",
       "      <td>welcome to the big time youre bound to be a s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pre-1980</td>\n",
       "      <td>once i believed that when love came to me it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000s</td>\n",
       "      <td>i took my love and i took it down climbed a m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pre-1980</td>\n",
       "      <td>do do do do do do do do do do do do do do do ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Era                                             Lyrics\n",
       "0  pre-1980  come on come on let me show you where its at a...\n",
       "1     1980s   welcome to the big time youre bound to be a s...\n",
       "2  pre-1980   once i believed that when love came to me it ...\n",
       "3     2000s   i took my love and i took it down climbed a m...\n",
       "4  pre-1980   do do do do do do do do do do do do do do do ..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bags of words\n",
    "\n",
    "Your first task is to convert the text to a bag-of-words representation. For this data, a lot of the preprocessing is already done: the text is lower-cased, and punctuation is removed. You need only create a `counter` for each instance.\n",
    "\n",
    "- **Deliverable 1.1**: Complete the function `gtnlplib.preproc.bag_of_words`. (0.25 points)\n",
    "- **Test**: `nose tests/test_preproc.py:test_d1_1_bow`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtnlplib import preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this block to update the notebook as you change the preproc library\n",
    "reload(preproc);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr,x_tr = preproc.read_data('lyrics-train.csv',preprocessor=preproc.bag_of_words)\n",
    "y_dv,x_dv = preproc.read_data('lyrics-dev.csv',preprocessor=preproc.bag_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_te,x_te = preproc.read_data('lyrics-test-hidden.csv',preprocessor=preproc.bag_of_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unseen words\n",
    "\n",
    "One challenge for classification is that words will appear in the test data that do not appear in the training data. Compute the number of words that appear in `lyrics-dev.csv`, but not in `lyrics-train.csv`. To do this, implement the following deliverables:\n",
    "\n",
    "- **Deliverable 1.2**: implement `gtnlplib.preproc.aggregate_counts`, a counter of all words in a list of bags-of-words.  (0.25 points)\n",
    "- **Deliverable 1.3**: implement `gtnlplib.preproc.compute_oov`, returning a list of words that appear in one list of bags-of-words, but not another.  (0.25 points)\n",
    "- **Tests**: `tests/test_preproc.py:test_d1_2_agg`, `tests/test_preproc.py:test_d1_3a_oov`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(preproc);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To write fast code, you can find bottlenecks using the %%timeit cell magic. \n",
    "\n",
    "Here I'm evaluating two different implementations of `aggregate_counts`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248 ms ± 10.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# print(len(x_tr))\n",
    "bleh = preproc.aggregate_counts(x_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245 ms ± 7.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "preproc.aggregate_counts_slow(x_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_dv = preproc.aggregate_counts(x_dv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the most common items in a counter by calling `counts.most_common()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('you', 5542), ('i', 5535), ('the', 5061), ('to', 3203), ('and', 2953)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_dv.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_tr = preproc.aggregate_counts(x_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2677"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preproc.compute_oov(counts_dv,counts_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30459"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preproc.compute_oov(counts_tr,counts_dv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.297246280257606"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproc.oov_rate(counts_dv,counts_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "30% of the words in the dev set do not appear in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Power laws\n",
    "\n",
    "Word count distributions are said to follow [power law](https://en.wikipedia.org/wiki/Power_law) distributions. \n",
    "\n",
    "In practice, this means that a log-log plot of frequency against rank is nearly linear. Let's see if this holds for our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x103109940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.loglog([val for word,val in counts_tr.most_common()])\n",
    "plt.loglog([val for word,val in counts_dv.most_common()])\n",
    "plt.xlabel('rank')\n",
    "plt.ylabel('frequency')\n",
    "plt.legend(['training set','dev set']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reflect**: The dataset we are working with does not include capitalization. How do you think this figure would change if capitalization distinctions were included?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning the vocabulary\n",
    "\n",
    "Let's prune the vocabulary to include only words that appear at least ten times in the training data.\n",
    "\n",
    "- **Deliverable 1.4:** Implement `preproc.prune_vocabulary` (0.25 points)\n",
    "- **Test**: `tests/test_preproc.py:test_d1_4_prune`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(preproc);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr_pruned, vocab = preproc.prune_vocabulary(counts_tr,x_tr,10)\n",
    "x_dv_pruned, _ = preproc.prune_vocabulary(counts_tr,x_dv,10)\n",
    "x_te_pruned, _ = preproc.prune_vocabulary(counts_tr,x_te,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4875"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88 79\n",
      "187 176\n"
     ]
    }
   ],
   "source": [
    "i = 94\n",
    "# print(x_dv[i])\n",
    "# print()\n",
    "# print()\n",
    "# print()\n",
    "# print( x_dv_pruned[i])\n",
    "print(len(x_dv[i]),len(x_dv_pruned[i]))\n",
    "print(sum(x_dv[i].values()),sum(x_dv_pruned[i].values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Linear classification\n",
    "\n",
    "Now you'll implement the linear classification rule, $\\hat{y} = \\text{argmax}_y \\theta^{\\top} f(x,y)$.\n",
    "\n",
    "You will use these functions in all classifiers in this assignment.\n",
    "\n",
    "Total: 2 points for 4650, 1 point for 7650."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtnlplib import clf_base\n",
    "reload(clf_base)\n",
    "\n",
    "from gtnlplib import constants\n",
    "reload(constants);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from class and the reading that the feature function vector $f(x,y)$ can be viewed as a dict, in which the values are counts, and the keys are tuples $(y,x_j)$, where $y$ is a label and $x_j$ is a base feature.\n",
    "\n",
    "- **Deliverable 2.1**: Implement the function ```make_feature_vector``` in ```clf_base.py```. (1 point for 4650, 0.5 points for 7650)\n",
    "- **Test**: `tests/test_classifier.py:test_d2_1_featvec`\n",
    "\n",
    "Note that you must also include the offset feature, ```gtnlplib.constants.OFFSET```.\n",
    "\n",
    "Desired output is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "fv = clf_base.make_feature_vector({'test':1,'case':2},'1980s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('1980s', 'test'): 1, ('1980s', 'case'): 2, ('1980s', '**OFFSET**'): 1}\n"
     ]
    }
   ],
   "source": [
    "print(fv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the entire set of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1990s', 'pre-1980', '1980s', '2000s'}\n"
     ]
    }
   ],
   "source": [
    "labels = set(y_tr) #figure out all possible labels\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now implement the prediction rule, $\\hat{y} = \\text{argmax}_y \\theta^{\\top} f(x,y)$.\n",
    "\n",
    "- **Deliverable 2.2**: Implement the function ```predict``` in ```clf_base.py```. (1 point for 4650, 0.5 points for 7650)\n",
    "- **Test**: `tests/test_classifier.py:test_d2_2_predict`\n",
    "\n",
    "The output should be:\n",
    "\n",
    "- A predicted label\n",
    "- The scores of all labels\n",
    "\n",
    "This function will be called **a lot**, so try to make it fast. You don't need to do anything crazy, but avoid making your code do silly extra work. It's worth trying out a couple different versions using %%timeit.\n",
    "\n",
    "You can test this function using these simple hand-crafted weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "reload(clf_base);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight vectors must be defaultdicts\n",
    "theta_hand = defaultdict(float,\n",
    "                         {('2000s','money'):0.1,\n",
    "                          ('2000s','name'):0.2,\n",
    "                          ('1980s','tonight'):0.1,\n",
    "                          ('2000s','man'):0.1,\n",
    "                          ('1990s','fly'):0.1,\n",
    "                          ('pre-1980', constants.OFFSET):0.1\n",
    "                         })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2000s',\n",
       " {'1980s': 0.0, '1990s': 0.0, '2000s': 1.3000000000000003, 'pre-1980': 0.1})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_base.predict(x_tr_pruned[0],theta_hand,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how good these weights are, by evaluating on the dev set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtnlplib import evaluation\n",
    "reload(evaluation);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3422222222222222\n"
     ]
    }
   ],
   "source": [
    "# this applies your predict function to all the instances in ```x_dv```\n",
    "y_hat = clf_base.predict_all(x_dv_pruned, theta_hand, labels)\n",
    "print(evaluation.acc(y_hat,y_dv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Naive Bayes\n",
    "\n",
    "You'll now implement a Naive Bayes classifier, as described in chapter 1 of the notes.\n",
    "\n",
    "Total: 2 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtnlplib import naive_bayes\n",
    "reload(naive_bayes);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Deliverable 3.1**: (warmup) implement ```get_corpus_counts``` in ```naive_bayes.py```. (0.5 points)\n",
    "- **Test**: `tests/test_classifier.py:test_d3_1_corpus_counts`\n",
    "\n",
    "This function should compute the word counts for a given label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "# print(type(x_tr_pruned), x_tr_pruned[0])\n",
    "eighties_counts = naive_bayes.get_corpus_counts(x_tr_pruned, y_tr, \"1980s\");\n",
    "print(eighties_counts['today'])\n",
    "print(eighties_counts['yesterday'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Deliverable 3.2**: Implement ```estimate_pxy``` in ```naive_bayes.py```. (0.5 points)\n",
    "- **Test**: `tests/test_classifier.py:test_d3_2_pxy`\n",
    "\n",
    "This function should compute the *smoothed* multinomial distribution $\\log P(x \\mid y)$ for a given label $y$.\n",
    "\n",
    "Hint: note that this function takes the vocabulary as an argument. You have to assign a probability even for words that do not appear in documents with label $y$, if they are in the vocabulary.\n",
    "\n",
    "You can use ```get_corpus_counts``` in this function if you want to, but you don't have to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_pxy = naive_bayes.estimate_pxy(x_tr_pruned,y_tr,\"1980s\",0.1,vocab);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probabilities must sum to one! (or very close)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999995085092385"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.exp(list(log_pxy.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the log-probabilities of the words from the hand-tuned weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'money': -7.6895632989076725, 'name': -7.568325205307788, 'tonight': -6.216638048498441, 'man': -6.63187743794878, 'fly': -8.636944617851858, '**OFFSET**': 0.0}\n"
     ]
    }
   ],
   "source": [
    "print({word:log_pxy[word] for (_,word),weight in theta_hand.items() if weight>0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_pxy_more_smooth = naive_bayes.estimate_pxy(x_tr_pruned,y_tr,\"1980s\",10,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'money': -7.801403237656476, 'name': -7.691200097522862, 'tonight': -6.405446965624849, 'man': -6.808511112195475, 'fly': -8.607490829397017, '**OFFSET**': 0.0}\n"
     ]
    }
   ],
   "source": [
    "print({word:log_pxy_more_smooth[word] for (_,word),weight in theta_hand.items() if weight>0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Deliverable 3.3**: Now you are ready to implement ```estimate_nb``` in ```naive_bayes.py```. (0.5 points)\n",
    "- **Test**: `tests/test_classifier.py:test_d3_3a_nb`\n",
    "\n",
    "\n",
    "\n",
    "- The goal is that the score given by ```clf_base.predict``` is equal to the joint probability $P(x,y)$, as described in the notes.\n",
    "- Don't forget the offset feature, whose weights should be set to the prior $\\log P(y)$.\n",
    "- The log-probabilities for the offset feature should not be smoothed.\n",
    "- You can call the functions you have defined above, but you don't have to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-202bdce53f5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnaive_bayes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtheta_nb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnaive_bayes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimate_nb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_tr_pruned\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Projects/CS-7650-Natural-Language/Problem-Set-1/gtnlplib/naive_bayes.py\u001b[0m in \u001b[0;36mestimate_nb\u001b[0;34m(x, y, smoothing)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mtheta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mestimate_pxy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmoothing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/CS-7650-Natural-Language/Problem-Set-1/gtnlplib/naive_bayes.py\u001b[0m in \u001b[0;36mestimate_pxy\u001b[0;34m(x, y, label, smoothing, vocab)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_of_words_in_docs_with_label\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msmoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0mlog_phi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount_aggregator_for_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msmoothing\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "reload(naive_bayes);\n",
    "theta_nb = naive_bayes.estimate_nb(x_tr_pruned,y_tr,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-40c2c164aae3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclf_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_tr_pruned\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m155\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtheta_nb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Projects/CS-7650-Natural-Language/Problem-Set-1/gtnlplib/clf_base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(base_features, weights, labels)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbase_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;31m# print(\"Label: \", label, \"Feature:\", feature)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbase_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOFFSET\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "clf_base.predict(x_tr_pruned[155],theta_nb,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = clf_base.predict_all(x_dv_pruned,theta_nb,labels)\n",
    "print(evaluation.acc(y_hat,y_dv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this block shows how we write and read predictions for evaluation\n",
    "evaluation.write_predictions(y_hat,'nb-dev.preds')\n",
    "y_hat_dv = evaluation.read_predictions('nb-dev.preds')\n",
    "evaluation.acc(y_hat_dv,y_dv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute this block to write predictions for the test set\n",
    "y_hat = clf_base.predict_all(x_te_pruned,theta_nb,labels)\n",
    "evaluation.write_predictions(y_hat,'nb-test.preds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can't run this, but this how the grading code works\n",
    "y_hat_te = evaluation.read_predictions('nb-test.preds')\n",
    "evaluation.acc(y_hat_te,y_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Deliverable 3.4**: Write a function in ```naive_bayes.py``` called ```find_best_smoother```, which finds the smoothing value that gives best performance on the dev data.  (0.5 points)\n",
    "- **Test**: `tests/test_classifier.py:test_d3_4a_nb_best`\n",
    "\n",
    "Your function should trying at least the following values in `vals` below.\n",
    "\n",
    "Then, using this smoothing value, run your Naive Bayes classifier on the test set, and output the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = np.logspace(-3,2,11)\n",
    "print(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_smoother, scores = naive_bayes.find_best_smoother(x_tr_pruned,y_tr,x_dv_pruned,y_dv,vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogx(list(scores.keys()),list(scores.values()),'o-');\n",
    "plt.xlabel('smoothing')\n",
    "plt.ylabel('dev set accuracy');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reflect:**\n",
    "\n",
    "- what might explain the dramatic drop in accuracy when the smoothing is increased from $10$ to $30$?\n",
    "- before you check, predict whether the accuracy will continue to significantly drop if you further increase the smoothing to $10000$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_nb = naive_bayes.estimate_nb(x_tr_pruned,y_tr,best_smoother)\n",
    "y_hat = clf_base.predict_all(x_te_pruned,theta_nb,labels)\n",
    "evaluation.write_predictions(y_hat,'nb-best-test.preds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can't run this\n",
    "y_hat = evaluation.read_predictions('nb-best-test.preds')\n",
    "print(evaluation.acc(y_hat,y_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Perceptron \n",
    "\n",
    "Total: 1.5 points\n",
    "\n",
    "The perceptron update is,\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{y} = & \\text{argmax}_y \\theta^\\top f(x,y)\\\\\n",
    "\\theta \\gets & \\theta + f(x,y) - f(x,\\hat{y})\n",
    "\\end{align}\n",
    "\n",
    "You will now implement this classifier, using the file ```gtnlplib/perceptron.py```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtnlplib import perceptron\n",
    "reload(perceptron);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Deliverable 4.1**: Implement the perceptron *update*, $f(x,y) - f(x,\\hat{y})$, in the function ```perceptron_update``` in ```perceptron.py```. (0.5 points)\n",
    "- **Test**: `tests/test_perceptron.py:test_d4_1_perc_update`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_perc = defaultdict(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-d63715a9df83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# no update when the prediction is correct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mupdate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperceptron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperceptron_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_tr_pruned\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta_perc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/CS-7650-Natural-Language/Problem-Set-1/gtnlplib/perceptron.py\u001b[0m in \u001b[0;36mperceptron_update\u001b[0;34m(x, y, weights, labels)\u001b[0m\n\u001b[1;32m     16\u001b[0m     '''\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# no update when the prediction is correct\n",
    "i=20\n",
    "update = perceptron.perceptron_update(x_tr_pruned[i], y_tr[i], theta_perc,labels)\n",
    "print(update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update when the prediction is incorrect\n",
    "i=110\n",
    "y_hat,_ = clf_base.predict(x_tr_pruned[i],theta_perc,labels)\n",
    "update =perceptron.perceptron_update(x_tr_pruned[i],y_tr[i],theta_perc,labels)\n",
    "print(list(update.items())[:5])\n",
    "print(len(update))\n",
    "print(y_tr[i],constants.OFFSET,update[((y_tr[i],constants.OFFSET))])\n",
    "print(y_hat,constants.OFFSET,update[((y_hat,constants.OFFSET))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update[(('1980s','with'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now implement the perceptron algorithm. Your implementation should take as inputs:\n",
    "\n",
    "- The training instances $x$\n",
    "- The training labels $y$\n",
    "- The number of iterations to train\n",
    "\n",
    "It should use your ```update``` function, and it should return:\n",
    "\n",
    "- weights $\\theta$\n",
    "- a list of the weights at each iteration\n",
    "\n",
    "\n",
    "- **Deliverable 4.2**: Implement ```estimate_perceptron``` in ```perceptron.py``` (1 point)\n",
    "- **Test**: `tests/test_perceptron.py:test_d4_2a_perc_estimate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(perceptron);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_perc,theta_perc_history = perceptron.estimate_perceptron(x_tr_pruned[:10],y_tr[:10],3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(theta_perc[('1980s','its')])\n",
    "print(theta_perc[('1980s','what')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, here is the running time on a relatively modern consumer-grade machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "theta_perc,theta_perc_history = perceptron.estimate_perceptron(x_tr_pruned,y_tr,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_perc,theta_perc_history = perceptron.estimate_perceptron(x_tr_pruned,y_tr,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this to plot the accuracy over iterations\n",
    "def plot_accs(weight_history,x_tr=x_tr_pruned,y_tr=y_tr,x_dv=x_dv_pruned,y_dv=y_dv):\n",
    "    tr_accs = []\n",
    "    dv_accs = []\n",
    "    for theta in weight_history:\n",
    "        tr_accs.append(evaluation.acc(clf_base.predict_all(x_tr,theta,labels),y_tr))\n",
    "        dv_accs.append(evaluation.acc(clf_base.predict_all(x_dv,theta,labels),y_dv))\n",
    "    plt.plot(tr_accs,'--')\n",
    "    plt.plot(dv_accs)\n",
    "    plt.xlabel('iteration')\n",
    "    plt.ylabel('accuracy');\n",
    "    plt.legend(['training','dev'],loc='lower right');\n",
    "    return tr_accs,dv_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = clf_base.predict_all(x_dv_pruned,theta_perc,labels)\n",
    "print(evaluation.acc(y_hat,y_dv));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accs(theta_perc_history);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute this code to write the predictions on the dev and training data\n",
    "y_hat_dv = clf_base.predict_all(x_dv_pruned,theta_perc,labels)\n",
    "evaluation.write_predictions(y_hat_dv,'perc-dev.preds')\n",
    "y_hat_te = clf_base.predict_all(x_te_pruned,theta_perc,labels)\n",
    "evaluation.write_predictions(y_hat_te,'perc-test.preds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = evaluation.read_predictions('perc-dev.preds')\n",
    "print(evaluation.acc(y_hat,y_dv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Logistic regression\n",
    "\n",
    "Total: 1.75 points\n",
    "\n",
    "You will implement logistic regression in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Converting data to numpy\n",
    "\n",
    "Numpy is a package for numerical computing in python.\n",
    "\n",
    "You will need to convert your bag-of-words list of counters to a numpy array. \n",
    "\n",
    "- **Deliverable 5.1**: Implement `preproc.py:make_numpy()` (0.5 points)\n",
    "- **Test**: `test_pytorch/test_d5_1_numpy`\n",
    "- **Hint**: one approach is to start with `numpy.zeros((height,width))`, and then fill in the cells by iterating through the bag-of-words list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((4,2))\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[1,1] = -1\n",
    "X[2,0] = 1.5\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(preproc);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr = preproc.make_numpy(x_tr_pruned,vocab)\n",
    "X_dv = preproc.make_numpy(x_dv_pruned,vocab)\n",
    "X_te = preproc.make_numpy(x_te_pruned,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_set = sorted(list(set(y_tr)))\n",
    "print(label_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_tr = np.array([label_set.index(y_i) for y_i in y_tr])\n",
    "Y_dv = np.array([label_set.index(y_i) for y_i in y_dv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(Y_tr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Building a logistic regression model\n",
    "\n",
    "- **Deliverable 5.2**: Complete `logreg.build_linear` (0.25 points)\n",
    "- **Test**: `tests/test_pytorch.py:test_d5_2_logreg`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtnlplib import logreg\n",
    "reload(logreg);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(765);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_var = Variable(torch.from_numpy(X_tr.astype(np.float32)))\n",
    "X_dv_var = Variable(torch.from_numpy(X_dv.astype(np.float32)))\n",
    "X_te_var = Variable(torch.from_numpy(X_te.astype(np.float32)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's always a good idea to check the dimensions of your data first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_tr_var.size())\n",
    "print(X_dv_var.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = logreg.build_linear(X_tr,Y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.forward(X_dv_var)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Adding a log-softmax layer\n",
    "\n",
    "We're done it for you in PyTorch. Write your own log-softmax function in numpy and verify the results.\n",
    "\n",
    "- **Deliverable 5.3**: Complete `logreg.log_softmax` (0.25 points)\n",
    "- **Test**: `tests/test_pytorch.py:test_d5_3_log_softmax`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_module('softmax',torch.nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.forward(X_dv_var)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.forward(X_dv_var)[:3].exp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that each row sums up to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.forward(X_dv_var)[:3].exp().sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(logreg);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.log_softmax(scores[:3].data.numpy()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These should be very close to the PyTorch results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Negative Log-Likelihood Loss\n",
    "\n",
    "A loss function tells you how well your model is doing. It produces gradients that allows the optimizer to tune the model weights. We've done the Pytorch call for you, try implementing this yourself in numpy!\n",
    "\n",
    "- **Deliverable 5.4**: Complete `logreg.nll_loss` (0.25 points)\n",
    "- **Test**: `tests/test_pytorch.py:test_d5_4_nll_loss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_tr_var = Variable(torch.from_numpy(Y_tr))\n",
    "Y_dv_var = Variable(torch.from_numpy(Y_dv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logP = model.forward(X_tr_var)\n",
    "print(loss.forward(logP,Y_tr_var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(logreg);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.nll_loss(logP.data.numpy(), Y_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, This should be very similar to the PyTorch result above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Putting everything together\n",
    "\n",
    "An optimizer can be used to actually learn the weights. We provide the complete code below that you can train on in `logreg.train_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(logreg);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a new model with a fixed seed\n",
    "torch.manual_seed(765)\n",
    "model = logreg.build_linear(X_tr,Y_tr)\n",
    "model.add_module('softmax',torch.nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trained, losses, accuracies = logreg.train_model(loss,model,\n",
    "                                                       X_tr_var,\n",
    "                                                       Y_tr_var,\n",
    "                                                       X_dv_var=X_dv_var,\n",
    "                                                       Y_dv_var = Y_dv_var,\n",
    "                                                       num_its=100,\n",
    "                                                       optim_args={'lr':0.02})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.plot_results(losses,accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 5.5**\n",
    "The noisy progress of the loss and dev set accuracy suggests that something is wrong with our hyperparameters. Tune the inputs to `train_model` until you can get to a dev set accuracy of at least 0.5. (0.5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a new model with a fixed seed\n",
    "torch.manual_seed(765)\n",
    "model = logreg.build_linear(X_tr,Y_tr)\n",
    "model.add_module('softmax',torch.nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trained, losses, accuracies = logreg.train_model(loss,model,\n",
    "                                                       X_tr_var,\n",
    "                                                       Y_tr_var,\n",
    "                                                       X_dv_var=X_dv_var,\n",
    "                                                       Y_dv_var = Y_dv_var,\n",
    "                                                       num_its=100,\n",
    "                                                       optim_args={'lr':0.02})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, Y_hat_dv = model_trained.forward(X_dv_var).max(dim=1)\n",
    "np.save('logreg-es-dev.preds.npy', Y_hat_dv.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = evaluation.acc(np.load('logreg-es-dev.preds.npy'),Y_dv_var.data.numpy())\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, Y_hat_te = model.forward(X_te_var).max(dim=1)\n",
    "np.save('logreg-es-test.preds.npy', Y_hat_te.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# You can't run this\n",
    "acc = evaluation.acc(np.load('logreg-es-test.preds.npy'),Y_te_var.data.numpy())\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Feature analysis\n",
    "\n",
    "Total: 1 point\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Top Features for Naive Bayes and Perceptron\n",
    "\n",
    "- **Deliverable 6.1**: Implement ```get_top_features_for_label_numpy``` in ```features.py```. (0.5 points)\n",
    "- **Test**: `tests/test_features.py:test_d6_1_topfeat_numpy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtnlplib import features\n",
    "reload(features);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.get_top_features_for_label_numpy(theta_perc,'pre-1980',7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.get_top_features_for_label_numpy(theta_perc,'1990s',7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Top Features for Logistic Regression\n",
    "\n",
    "- **Deliverable 6.2**: Implement ```get_top_features_for_label_torch``` in ```features.py```. (0.5 points)\n",
    "- **Test**: `tests/test_features.py:test_d6_2_topfeat_torch`\n",
    "\n",
    "**Hint**: Extract linear layer weights from the PyTorch model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(features);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a fixed model so we have reproducible results. Feel free to change it to your own model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test = torch.load('tests/test_weights.torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.get_top_features_for_label_torch(model_test, vocab, label_set,'pre-1980',5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.get_top_features_for_label_torch(model_test, vocab, label_set,'1990s',7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Feature Engineering\n",
    "\n",
    "Total: 0.75 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Example Feature - Token-Type Ratio\n",
    "\n",
    "You can have features other than bag of words in your model. For example, we can consider the token-type ratio for each song. The token-type ratio is:\n",
    "\\begin{equation}\n",
    "\\frac{\\text{length of song in tokens}}{\\text{number of distinct types}} = \\frac{\\sum_m w_m}{\\sum_m \\delta(w_m > 0)}\n",
    "\\end{equation}\n",
    "\n",
    "- **Deliverable 7.1**: Implement ```get_token_type_ratio``` in ```features.py```. (0.25 points)\n",
    "- **Test**: `tests/test_features.py:test_d7_1_token_type_ratio`\n",
    "\n",
    "Return zero if the length of the song is zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(features);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Token-type ratios for the first five songs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[features.get_token_type_ratio(X_tr[i]) for i in range(5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Binning Your Features\n",
    "\n",
    "Discretize your token-type ratio feature into bins: \n",
    "\n",
    "\n",
    "$\\{ [0,1), [1,2), [2,3), [3,4), [4,5), [5,6), [6, \\infty) \\}$.\n",
    "\n",
    "For each instance, there will be seven new features (one per bin). Exactly one of these features will have the value one; all others will have the value zero.\n",
    "\n",
    "Use `np.concatenate` or `np.hstack` to concatenate your result to the variable X_tr.\n",
    "\n",
    "- **Deliverable 7.2**: Implement ```concat_ttr_binned_features``` in ```features.py```. (0.5 points)\n",
    "- **Test**: `tests/test_features.py:test_d7_2_discretize`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(features);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_new = features.concat_ttr_binned_features(X_tr)\n",
    "print(X_tr_new)\n",
    "print(X_tr_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dv_new = features.concat_ttr_binned_features(X_dv)\n",
    "X_te_new = features.concat_ttr_binned_features(X_te)\n",
    "X_tr_var = Variable(torch.from_numpy(X_tr_new.astype(np.float32)))\n",
    "X_dv_var = Variable(torch.from_numpy(X_dv_new.astype(np.float32)))\n",
    "X_te_var = Variable(torch.from_numpy(X_te_new.astype(np.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = logreg.build_linear(X_tr_var,Y_tr)\n",
    "model.add_module('softmax',torch.nn.LogSoftmax(dim=1))\n",
    "loss = torch.nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See if these features help!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model,losses,accuracies = logreg.train_model(loss,model,X_tr_var,Y_tr_var,\n",
    "                                             Y_dv_var=Y_dv_var,X_dv_var = X_dv_var,\n",
    "                                             num_its=500,status_frequency=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.plot_results(losses,accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Bakeoff\n",
    "\n",
    "**Deliverable 7.3**: Try to get the best accuracy possible. \n",
    "\n",
    "Some ideas:\n",
    "\n",
    "- Better features\n",
    "- Better optimization\n",
    "- Better classifier, e.g. multilayer neural networks\n",
    "- Better loss function\n",
    "- Better preprocessing\n",
    "- Dropout or other regularization scheme\n",
    "\n",
    "The current best accuracies from the staff are 55.5% dev, 59.3% test.\n",
    "\n",
    "### Rubric\n",
    "\n",
    "Dev set\n",
    "- $\\geq 55\\%$: 1 point\n",
    "- $\\geq 54\\%$: 0.75 points\n",
    "- $\\geq 53\\%$: 0.5 points\n",
    "- $\\geq 51.5\\%$: 0.25 points\n",
    "\n",
    "Test set\n",
    "- $\\geq 58\\%$: 1 point\n",
    "- $\\geq 55\\%$: 0.75 points\n",
    "- $\\geq 52\\%$: 0.5 points\n",
    "- $\\geq 50\\%$: 0.25 points\n",
    "    \n",
    "### Extra credit\n",
    "- We will run a Kaggle competition for this bakeoff. More details are coming soon.\n",
    "- Extra credit will be given to the top three submissions (combined across 4650/7650), by **test set** performance: 1 point, 0.75 points, 0.5 points. \n",
    "- Another 1 point of extra credit will be awarded to submissions that are better than the best staff system, on the test set.\n",
    "- Staff will continue to try to improve their results until the deadline, but we will not tune on test set accuracy.\n",
    "- **Extra credit will be based on Kaggle submissions.** You don't have to participate in the Kaggle part of the bakeoff, but only Kaggle submissions will be eligible for extra credit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, Y_hat_dv = model.forward(X_dv_var).max(dim=1)\n",
    "np.save('bakeoff-dev.preds.npy', Y_hat_dv.data.numpy())\n",
    "evaluation.acc(np.load('bakeoff-dev.preds.npy'), Y_dv_var.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, Y_hat_te = model.forward(X_te_var).max(dim=1)\n",
    "np.save('bakeoff-test.preds.npy', Y_hat_te.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can't run this\n",
    "evaluation.acc(np.load('bakeoff-test.preds.npy'), Y_te_var.data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. 7650 Research Question\n",
    "\n",
    "(1 point.) CS 4650 students may do this component if they want; if they do, then the assignment will be graded as if they are in 7650. This is optional for CS4650 students, but if you submit something for this part, that is how you will be scored -- we're not taking the max over the two possible scoring options. CS 7650 students must do this part.\n",
    "\n",
    "You will select a recent research paper that performs *document* classification, using text. Summarize the paper, answering the following questions:\n",
    "\n",
    "- What are the labels, and how were they obtained?\n",
    "- Why is it interesting/useful to predict these labels?  \n",
    "- What classifier(s) do they use, and the reasons behind their choice? Do they use linear classifiers like the ones in this problem set?\n",
    "- What features do they use? Explain any features outside the bag-of-words model, and why they used them.\n",
    "- What is the conclusion of the paper? Do they compare between classifiers, between feature sets, or on some other dimension? \n",
    "- Give a one-sentence summary of the message that they are trying to leave for the reader.\n",
    "\n",
    "Your selection of papers is determined by the last digit of your GTID.\n",
    "\n",
    "- Digits 0-4: choose from ACL 2017, AAAI 2017, EACL 2017\n",
    "- Digits 5-9: choose from NAACL 2017, KDD 2017, EMNLP 2017\n",
    "\n",
    "You must choose a paper in the main conference (not workshops). The paper must be at least four pages long. All papers from these conferences are available for free online."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
