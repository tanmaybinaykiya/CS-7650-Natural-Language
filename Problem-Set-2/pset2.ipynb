{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem Set 2: Sequence Labeling\n",
    "=======================\n",
    "\n",
    "- This problem set focuses on sequence labeling with Hidden Markov models and Deep Learning Models.\n",
    "- The target domain is part-of-speech tagging on English and Norwegian from the Universal Dependencies dataset\n",
    "\n",
    "You will:\n",
    "- Do some basic preprocessing of the data\n",
    "- Build a naive classifier that tags each word with its most common tag\n",
    "- Implement a `Viterbi` Tagger using `Hidden Markov Model` in PyTorch\n",
    "- Build a `Bi-LSTM` deep learning model using PyTorch\n",
    "- Build a `Bi-LSTM_CRF` model using the above components (`Viterbi` and `Bi-LSTM`) \n",
    "- then implement techniques to improve your classifier and compete on Kaggle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setup\n",
    "\n",
    "In order to develop this assignment, you will need [python 3.6](https://www.python.org/downloads/) and the following libraries. Most if not all of these are part of [anaconda](https://www.continuum.io/downloads), so a good starting point would be to install that.\n",
    "\n",
    "- [jupyter](http://jupyter.readthedocs.org/en/latest/install.html)\n",
    "- [numpy](https://docs.scipy.org/doc/numpy/user/install.html)\n",
    "- [matplotlib](http://matplotlib.org/users/installing.html)\n",
    "- [nosetests](https://nose.readthedocs.org/en/latest/)\n",
    "- [pandas](http://pandas.pydata.org/) Dataframes\n",
    "\n",
    "Here is some help on installing packages in python: https://packaging.python.org/installing/. You can use ```pip --user``` to install locally without sudo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this assignment\n",
    "\n",
    "- This is a Jupyter notebook. You can execute cell blocks by pressing control-enter.\n",
    "- Most of your coding will be in the python source files in the directory ```gtnlplib```.\n",
    "- The directory ```tests``` contains unit tests that will be used to grade your assignment, using ```nosetests```. You should run them as you work on the assignment to see that you're on the right track. You are free to look at their source code, if that helps -- though most of the relevant code is also here in this notebook. Learn more about running unit tests at http://pythontesting.net/framework/nose/nose-introduction/\n",
    "- You may want to add more tests, but that is completely optional. \n",
    "- **To submit this assignment, run the script ```make-submission.sh```, and submit the tarball ```pset2-submission.tgz``` on Canvas.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Python version\n",
      "python: 3.6.4 (default, Dec 19 2017, 15:24:51) \n",
      "[GCC 4.2.1 Compatible Apple LLVM 9.0.0 (clang-900.0.39.2)]\n"
     ]
    }
   ],
   "source": [
    "print('My Python version')\n",
    "\n",
    "print('python: {}'.format(sys.version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nose\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My library versions\n",
      "pandas: 0.22.0\n",
      "numpy: 1.13.3\n",
      "matplotlib: 2.1.2\n",
      "nose: 1.3.7\n",
      "torch: 0.3.1\n"
     ]
    }
   ],
   "source": [
    "print('My library versions')\n",
    "\n",
    "print('pandas: {}'.format(pd.__version__))\n",
    "print('numpy: {}'.format(np.__version__))\n",
    "print('matplotlib: {}'.format(matplotlib.__version__))\n",
    "print('nose: {}'.format(nose.__version__))\n",
    "print('torch: {}'.format(torch.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test whether your libraries are the right version, run:\n",
    "\n",
    "`nosetests tests/test_environment.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\r\n",
      "----------------------------------------------------------------------\r\n",
      "Ran 1 test in 0.001s\r\n",
      "\r\n",
      "OK\r\n"
     ]
    }
   ],
   "source": [
    "# use ! to run shell commands in notebook\n",
    "! nosetests tests/test_environment.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "# importing all the necessary files from gtnlplib\n",
    "from gtnlplib import constants, preproc, most_common, clf_base, evaluation\n",
    "from gtnlplib import scorer, tagger_base, naive_bayes, hmm, viterbi, bilstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data Processing\n",
    "===================\n",
    "The part-of-speech tags are defined on the [universal dependencies website](http://universaldependencies.org/en/pos/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(constants);\n",
    "## Define the file names\n",
    "TRAIN_FILE = constants.TRAIN_FILE\n",
    "DEV_FILE = constants.DEV_FILE\n",
    "TEST_FILE = constants.TEST_FILE \n",
    "TEST_FILE_HIDDEN = constants.TEST_FILE_UNLABELED\n",
    "NR_TRAIN_FILE = constants.NR_TRAIN_FILE\n",
    "NR_DEV_FILE = constants.NR_DEV_FILE\n",
    "NR_TEST_FILE = constants.NR_TEST_FILE\n",
    "NR_TEST_FILE_HIDDEN = constants.NR_TEST_FILE_UNLABELED\n",
    "# change the constant to test_file_unlabeled for release"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here is demo code for using the function `conll_seq_generator(...)`. \n",
    "- The default value for max_insts is `1000000` indicating the num. of instances: and this should be enough for our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ADJ', 'ADP', 'ADV', 'AUX', 'CONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X']\n"
     ]
    }
   ],
   "source": [
    "## Demo\n",
    "all_tags = set([])\n",
    "for (words, tags) in preproc.conll_seq_generator(TRAIN_FILE):\n",
    "    for tag in tags:\n",
    "        all_tags.add(tag)\n",
    "all_tags = sorted(all_tags)\n",
    "print (all_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 1.1**: Counting words per tag. (* 1 point *)\n",
    "\n",
    "Implement the function `get_tag_word_counts` in `most_common.py`: The function should calculate the number of occurrences of all words for each tag.\n",
    "\n",
    "- **Input**: filename for data file, to be passed as argument to `preproc.conll_seq_generation`\n",
    "- **Output**: dict of counters, where keys are tags\n",
    "- **Tests**: ```test_most_common.py: test_get_top_noun_tags(),test_get_top_verb_tags()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(most_common);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROPN [('Bush', 100), ('US', 81), ('Al', 78)]\n",
      "PUNCT [(',', 1616), ('.', 1588), ('\"', 383)]\n",
      "ADJ [('other', 46), ('many', 41), ('Indian', 35)]\n",
      "NOUN [('people', 53), ('time', 48), ('world', 46)]\n",
      "VERB [('is', 335), ('was', 128), ('have', 110)]\n",
      "DET [('the', 1926), ('a', 650), ('The', 216)]\n",
      "ADP [('of', 887), ('in', 738), ('to', 380)]\n",
      "AUX [('have', 139), ('was', 126), ('has', 124)]\n",
      "PRON [('I', 251), ('it', 208), ('he', 131)]\n",
      "PART [('to', 542), (\"'s\", 218), ('not', 172)]\n",
      "SCONJ [('that', 304), ('if', 56), ('as', 47)]\n",
      "NUM [('one', 52), ('two', 28), ('2001', 17)]\n",
      "ADV [('also', 63), ('now', 54), ('when', 53)]\n",
      "CONJ [('and', 932), ('or', 127), ('but', 88)]\n",
      "X [('1', 3), ('2', 3), ('3', 3)]\n",
      "INTJ [('Please', 15), ('please', 3), ('Well', 3)]\n",
      "SYM [('$', 20), ('-', 13), ('/', 7)]\n"
     ]
    }
   ],
   "source": [
    "# this block uses your code to find the three most common words per tag\n",
    "counters = most_common.get_tag_word_counts(TRAIN_FILE)\n",
    "for tag,tag_ctr in counters.items():\n",
    "    print (tag,tag_ctr.most_common(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2. Tagging as classification \n",
    "\n",
    "Now you will implement part-of-speech tagging via classification.\n",
    "\n",
    "Tagging quality is evaluated using evalTagger, which takes three arguments:\n",
    "- a tagger, which is a **function** taking a list of words and a tagset as arguments and returns the predicted tags for the words\n",
    "- an output filename\n",
    "- a test file\n",
    "\n",
    "You will want to use lambda expressions to create the first argument for the `eval_tagger(..)` function, as shown below.\n",
    "Here's how it works. I provide a tagger that labels everything as a noun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(tagger_base);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1527613022274944\n"
     ]
    }
   ],
   "source": [
    "# here is a tagger that just tags everything as a noun\n",
    "noun_tagger = lambda words, alltags : ['NOUN' for word in words]\n",
    "\n",
    "confusion = tagger_base.eval_tagger(noun_tagger,'nouns.preds',all_tags=all_tags)\n",
    "print (scorer.accuracy(confusion))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "** Deliverable 2.1 ** Classification-based tagging. (* 0.5 points *)\n",
    "\n",
    "Now do the same thing as above, but building your tagger *as a classifier.* To do this, implement `make_classifier_tagger()` in `tagger_base.py`. \n",
    "\n",
    "- **Input**: defaultdict of weights\n",
    "- **Output**: return a function that takes in (list of word tokens, list of all possible tags) $\\rightarrow$ tags for each word\n",
    "\n",
    "The function that you output should create the base-features for each token (**use the OFFSET and the TOKEN itself as base-features**)  and then use your `clf_base.predict()` function from pset 1. You are free to edit the `clf_base.predict()` function if you don't think you got it right in pset 1.\n",
    "- **Tests**: ```test_classifier_tagger.py:test_classifier()```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(tagger_base);\n",
    "reload(clf_base);\n",
    "from gtnlplib.constants import OFFSET #OFFSET token is for each tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now create a tagger with weights that predict every token to be a NOUN. \n",
    "The function `get_noun_weights` is already implemented for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_noun_tagger = tagger_base.make_classifier_tagger(most_common.get_noun_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1527613022274944\n"
     ]
    }
   ],
   "source": [
    "confusion = tagger_base.eval_tagger(classifier_noun_tagger,'all-nouns.preds',all_tags=all_tags)\n",
    "print (scorer.accuracy(confusion))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Deliverable 2.2** Tagging words by their most common tag. (* 0.5 points *)\n",
    "\n",
    "Now build a classifier tagger that tags each word with its most common tag in the training set. To do this, implement `get_most_common_word_weights` in `most_common.py`.  \n",
    "\n",
    "- **Input**: training file\n",
    "\n",
    "- **Output**: defaultdict of weights\n",
    "\n",
    "This function should return a set weights such that each word should get the tag that is most frequently associated with it in the training data. If the word does not appear in the training data, the weights should be set so that the tagger outputs the **most common tag** in the training data. For the out of vocabulary words, you need to think on how to set the weights so that you tag them by the most common tag.\n",
    "\n",
    "- **Tests**: ```test_classifier.py:test_mcc_tagger_output(), test_mcc_tagger_accuracy()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "reload(most_common);\n",
    "reload(tagger_base);\n",
    "reload(clf_base);\n",
    "\n",
    "theta_mc = most_common.get_most_common_word_weights(TRAIN_FILE)\n",
    "print(theta_mc[('ADJ', 'tanmay')])\n",
    "print(theta_mc[('NOUN', 'tanmay')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger_mc = tagger_base.make_classifier_tagger(theta_mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PRON', 'AUX', 'AUX', 'NOUN']\n"
     ]
    }
   ],
   "source": [
    "tags = tagger_mc(['They','can','can','fish'],all_tags)\n",
    "print (tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DET', 'ADJ', 'NOUN', 'DET', 'PROPN', 'PUNCT']\n"
     ]
    }
   ],
   "source": [
    "tags = tagger_mc(['The','old','man','the','boat','.'],all_tags)\n",
    "print (tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Now let's run your tagger on the dev data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8120469223672071\n"
     ]
    }
   ],
   "source": [
    "confusion = tagger_base.eval_tagger(tagger_mc,'most-common.preds',all_tags=all_tags)\n",
    "print (scorer.accuracy(confusion))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Naive Bayes as a tagger.\n",
    "    \n",
    "- You can use your Naive Bayes classifier form pset1 to set the weights for the classifier tagger. We have added in a helper function `naive_bayes.get_nb_weights(..)`. Make sure to retain this function, when you copy your code.\n",
    "- If you don't think you got it right in pset1, you are free to change it now. If you got it right, then just examine the performance of naive bayes as tagger in the following blocks. There is no test or deliverable for this part.\n",
    "- Note that, for text classification, we had a bag of words feature vector and label for each document. For POS tagging, in order to estimate the weights for the classifier tagger, we will consider each token to be its own document. The following helper code converts the dataset to token level bag-of-words feature vector and labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(naive_bayes);\n",
    "nb_weights = naive_bayes.get_nb_weights(TRAIN_FILE, .01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- This gives weights for each tag-word pair that represent $\\log P(word \\mid tag)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtaining vocab of words\n",
    "vocab = set([word for tag,word in nb_weights.keys() if word is not constants.OFFSET])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6912\n"
     ]
    }
   ],
   "source": [
    "print (len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print (sum(np.exp(nb_weights[('ADJ',word)]) for word in vocab))\n",
    "print (sum(np.exp(nb_weights[('NOUN',word)]) for word in vocab))\n",
    "print (sum(np.exp(nb_weights[('PUNCT',word)]) for word in vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- We have zero weights for OOV terms -- think about how this affects the classification here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print (nb_weights[('ADJ','baaaaaaaaad')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.13147297137\n",
      "-3.09779855448\n",
      "-2.80928210568\n"
     ]
    }
   ],
   "source": [
    "print (nb_weights[('VERB',constants.OFFSET)])\n",
    "print (nb_weights[('ADV',constants.OFFSET)])\n",
    "print (nb_weights[('PRON',constants.OFFSET)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Offsets should correspond to log-probabilities $\\log P(y)$ such that $\\sum_y P(y) = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99999999999999978"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.exp(nb_weights[(tag,constants.OFFSET)]) for tag in all_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now let us look at the accuracy of our naive_bayes tagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3681296955318308\n"
     ]
    }
   ],
   "source": [
    "confusion = tagger_base.eval_tagger(tagger_base.make_classifier_tagger(nb_weights),'nb-simple.preds')\n",
    "dev_acc = scorer.accuracy(confusion)\n",
    "print (dev_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Just as good as the heuristic tagger from above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Viterbi Algorithm\n",
    "\n",
    "In this section you will implement the Viterbi algorithm in **PyTorch**. To get warmed up, let's work out an example by hand. For simplicity, there are only two tags, **N**OUN and **V**ERB. Here are the parameters:\n",
    "\n",
    "| | Value |\n",
    "| ------------- |:-------------:|\n",
    "| $\\log P_E(\\cdot|N)$ | they: -1, can: -3, fish: -3 |\n",
    "| $\\log P_E(\\cdot|V)$ | they: -11, can: -2, fish: -4 |\n",
    "| $\\log P_T(\\cdot|N)$ | N: -5, V: -2, END: -2 |\n",
    "| $\\log P_T(\\cdot|V)$ | N: -1, V: -3, END: -3 |\n",
    "| $\\log P_T(\\cdot|\\text{START})$ | N :-1, V :-2 |\n",
    "\n",
    "where $P_E(\\cdot|\\cdot)$ is the emission probability and $P_T(\\cdot|\\cdot)$ is the transition probability.\n",
    " \n",
    "- In class we discussed the sentence *They can fish*. \n",
    "- Now work out a more complicated example: \"*They can can fish*\", where the second \"*can*\" refers to the verb of putting things into cans.\n",
    " \n",
    "** Deliverable 3.1 ** Work out the trellis by hand, and fill in the table in the file **```text-answers.md```** (*0.5 points*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Viterbi ##\n",
    "\n",
    "Here are some predefined weights, corresponding to the weights from the problem 3.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TAG = constants.START_TAG\n",
    "END_TAG = constants.END_TAG\n",
    "UNK = constants.UNK\n",
    "\n",
    "nb_weights={('NOUN','they'):-1,\\\n",
    "            ('NOUN','can'):-3,\\\n",
    "            ('NOUN','fish'):-3,\\\n",
    "            ('VERB','they'):-11,\\\n",
    "            ('VERB','can'):-2,\\\n",
    "            ('VERB','fish'):-4,}\n",
    "hmm_trans_weights={('NOUN','NOUN'):-5,\\\n",
    "                   ('VERB','NOUN'):-2,\\\n",
    "                   (END_TAG,'NOUN'):-2,\\\n",
    "                   ('NOUN','VERB'):-1,\\\n",
    "                   ('VERB','VERB'):-3,\\\n",
    "                   (END_TAG,'VERB'):-3,\\\n",
    "                   ('NOUN',START_TAG):-1,\\\n",
    "                   ('VERB',START_TAG):-2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Deliverable 3.2 ** Complete the ```hmm.compute_weights_variables(...)``` function in `hmm.py` file (*0.5 points*) The function should basically convert the weights to respective pytorch variables .\n",
    "\n",
    "- **Inputs** :\n",
    "    - `nb_weights`: emission_weights: dictionary of weights\n",
    "    - `hmm_trans_weights`: dictionary of weights\n",
    "    - `vocab`: list of all the words\n",
    "    - `word_to_ix`: a dictionary that maps each word in the vocab to a unique index. **Does not have the OFFSET_TOKEN.**\n",
    "    - `tag_to_ix`: a dictionary that maps each tag (including the `START_TAG` and the `END_TAG`) to a unique index.  \n",
    "\n",
    "- **Outputs** : returns two torch Variables\n",
    "    - `emission_probs`: torch Variable of a matrix of size `Vocab x Tagset_size`: \n",
    "        such that for a specific weight say `(word1, tag1):value` would result in\n",
    "        `emission_probs[word_to_ix[word1]][tag_to_ix[tag1]]=value`, else a zero. Also, make sure to set weights such that `START_TAG` and `END_TAG` cannot generate any word. **Make sure to ignore the OFFSET weights that might be present in the nb_weights. Consider the words only in your word_to_ix.**\n",
    "        \n",
    "    - `tag_transition_probs`: torch Variable of a matrix of size `Tagset_size x Tagset_size`: \n",
    "        such that for a specific feature say `(tag1, tag2):value` \n",
    "        where tag1 is my succeeding tag, tag2 is my current tag. \n",
    "        This would result in `tag_transition_probs[tag_to_ix[tag1]][tag_to_ix[tag2]]=value`. \n",
    "        Also ensure to set the other weights such that there are no illegal transitions \n",
    "        (like from some tag to START_TAG -or- from END_TAG to some other tag)  \n",
    "\n",
    "- **Tests**: ```test_viterbi.py: test_compute_hmm_weights_variables()```\n",
    "\n",
    "Hint: Use `-np.inf` as weights for illegal transitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Below, observe that we are calculating `tag_to_ix, ix_to_tag, word_to_ix`. These are useful to access a particular emission score for a particular token and a tag. Look through the variables: tag_transition_probs and emission_probs below and it should be clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(hmm);\n",
    "word_to_ix={'they':0, 'can':1, 'fish':2, UNK:3}\n",
    "tag_to_ix = {START_TAG:0, 'NOUN':1, 'VERB':2, END_TAG:3}\n",
    "ix_to_tag = {v:k for k,v in tag_to_ix.items()}\n",
    "all_tags = [START_TAG, 'NOUN', 'VERB', END_TAG]\n",
    "words = ['they', 'can', 'fish']\n",
    "vocab = ['they','can','fish',UNK]\n",
    "# note that we are also including an UNK token: this will be helpful later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emission_probs, tag_transition_probs = hmm.compute_weights_variables(nb_weights, hmm_trans_weights, vocab, \n",
    "                                                                     word_to_ix, tag_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (tag_transition_probs)\n",
    "# tag_transition_probs[0] corresponds to scores for START_TAG from START_TAG, NOUN, VERB, END_TAG\n",
    "# tag_transition_probs[1] corresponds to scores for NOUN from START_TAG, NOUN, VERB, END_TAG\n",
    "# tag_transition_probs[2] corresponds to scores for VERB from START_TAG, NOUN, VERB, END_TAG\n",
    "# tag_transition_probs[3] corresponds to scores for END_TAG from START_TAG, NOUN, VERB, END_TAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (emission_probs)\n",
    "# emission_probs[0] corresponds to scores for the token 'they' for START_TAG, NOUN, VERB, END_TAG\n",
    "# emission_probs[1] corresponds to scores for the token 'can' for START_TAG, NOUN, VERB, END_TAG\n",
    "# emission_probs[2] corresponds to scores for the token 'fish' for START_TAG, NOUN, VERB, END_TAG\n",
    "# emission_probs[2] corresponds to scores for the token 'UNK' for START_TAG, NOUN, VERB, END_TAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we will be using these emission scores as inputs for each token in the input in the following function: ```viterbi_step()```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 3.3** The Viterbi recurrence. (*0.5 point*)\n",
    "\n",
    "Implement `viterbi_step` in `gtnlplib/viterbi.py`. This is the method that will compute the best path score and corresponding back pointer for a particular token in the sentence for all possible tags, which you will later call from the main viterbi routine. You will also be using this later for the `Bi-LSTM_CRF` model.\n",
    "\n",
    "### Inputs\n",
    "- `all_tags`: list of all tags: includes both the `START_TAG` and the `END_TAG`\n",
    "- `tag_to_ix`: a dictionary that maps each tag (including the `START_TAG` and the `END_TAG`) to a unique index: this is useful to access the respective tag transition scores from the tag_transition_probs variable.\n",
    "- `cur_tag_scores`: pytorch Variable that contains the local emission score for each tag for the current token in the sentence.\n",
    "    - `cur_tag_scores` size is : `[ len(all_tags) ] `\n",
    "- `transition_scores`: pytorch Variable that contains the `tag_transition_scores`. \n",
    "    - `transition_scores` size is : `[ len(all_tags) x len(all_tags) ]` \n",
    "- `prev_scores`: pytorch Variable that contains the scores for each tag for the previous token in the sentence.\n",
    "    - `prev_scores` size is : `[ 1 x len(all_tags) ] `\n",
    "\n",
    "### Outputs\n",
    "- `viterbivars`: a pytorch Variable that contains the global scores for each tag for the current token in the sentence\n",
    "- `bptrs`: a list of idx that contains the best_previous_tag for each tag for the current token in the sentence\n",
    "\n",
    "### Tests\n",
    "- ```test_viterbi.py: test_viterbi_step_init()```  \n",
    "\n",
    "There are a lot of inputs, but the code itself will not be very complex. Make sure you understand what each input represents before starting to write a solution.\n",
    "\n",
    "**Do not convert the pytorch variables into numpy. You will be using this function in BiLSTM-CRF model later on and you need the computation graph to be intact in order to backpropagate.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Consider the sentence: `'they can can fish'`\n",
    "- Let us observe the viterbi scores at each of the tokens 'they', 'can', 'can', 'fish'.\n",
    "- We will walk through this example and all along: these scores should match with the scores you obtained when you worked it out by hand.\n",
    "- **Please note the dimensions of the tensors below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(viterbi);\n",
    "initial_vec = np.full((1, len(all_tags)),-np.inf) \n",
    "initial_vec[tag_to_ix[START_TAG]][0] = 0 #setting all the score to START_TAG\n",
    "prev_scores = viterbi.get_torch_variable(initial_vec)\n",
    "# these are the previous scores for each_tag: START_TAG, NOUN, VERB, END_TAG\n",
    "print (prev_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The block above says that the only possible previous tag at $m=1$ is `START_TAG`\n",
    "- Now let us look at the tag scores for the first token 'they'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure both START_TAG and END_TAG is included in all_tags\n",
    "print (all_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Carefully observe all the inputs to the `viterbi_step(..)` function here.\n",
    "    - `all_tags`: is the list of all possible tags here\n",
    "    - `tag_to_ix`: a mapping from tags to unique ids: this is useful to access the respective tag transition scores from the `tag_transition_probs` variable.\n",
    "    - `cur_tag_scores`: observe that from previous section: `emission_probs` indicates the emission scores for each tag for each word they: since we will be tagging the word 'they' right now in our example: we will be using ```emission_probs[0]```: note that '0' is the id for our word 'they'.Thus, we send in `emission_probs[0]` as our cur_tag_scores\n",
    "    - `tag_transition_probs`: tag transition probabilities\n",
    "    - `prev_scores`: prev_scores obtained: we have initially calculated these scores above such that the `START_TAG` has all the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(viterbi);\n",
    "viterbivars, bptrs = viterbi.viterbi_step(all_tags, tag_to_ix, \n",
    "                                          emission_probs[0], \n",
    "                                          tag_transition_probs,\n",
    "                                          prev_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The following are the scores obtained for each tag for the word token 'they' and the backpointer refers to that particular previous tag which resulted in that score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = viterbivars\n",
    "for k,v in tag_to_ix.items():\n",
    "    print ('tag: ',k, ' score: ',scores[v].data.numpy(), ' back-pointer-tag: ', \n",
    "           ix_to_tag[bptrs[v]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `NOUN` has the highest score for the current tag, and its backpointer is to `START_TAG`\n",
    "- Now, let us look at the scores for the tags for the second token 'can'. Send in `emission_probs[1]` as our `current_tag_scores`, and update `prev_scores` to be the scores obtained for $m=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prev_scores = viterbi.get_torch_variable([-np.inf, -2, -13, -np.inf])\n",
    "viterbivars, bptrs = viterbi.viterbi_step(all_tags, tag_to_ix,\n",
    "                                          emission_probs[1],\n",
    "                                          tag_transition_probs,\n",
    "                                          prev_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The following are the scores obtained for each tag for the word token 'can' and its respective back_pointer tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = viterbivars\n",
    "for k,v in tag_to_ix.items():\n",
    "    print ('tag: ',k, ' score: ',scores[v].data.numpy(), ' back-pointer-tag: ',\n",
    "           ix_to_tag[bptrs[(all_tags).index(k)]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now, Below, let us look at the scores for the tags for the third token 'can'. So, now we send in `emission_probs[1]` as our `current_tag_scores` and we update `prev_scores` to be the scores obtained for the previous token 'can'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_scores = viterbi.get_torch_variable([-np.inf, -10, -6, -4]) \n",
    "viterbivars, bptrs = viterbi.viterbi_step(all_tags, tag_to_ix,\n",
    "                                          emission_probs[1],\n",
    "                                          tag_transition_probs,\n",
    "                                          prev_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = viterbivars\n",
    "for k,v in tag_to_ix.items():\n",
    "    print ('tag: ',k, ' score: ',scores[v].data.numpy(), ' back-pointer-tag: ',\n",
    "           ix_to_tag[bptrs[(all_tags).index(k)]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Now, let us look at the scores for the tags for the last token 'fish', So, now we send in `emission_probs[2]` as our `current_tag_scores` and we update `prev_scores` to be the scores obtained above for the previous token 'can'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_scores = viterbi.get_torch_variable([-np.inf, -10, -11, -np.inf])\n",
    "viterbivars, bptrs = viterbi.viterbi_step(all_tags, tag_to_ix,\n",
    "                                           emission_probs[2],\n",
    "                                           tag_transition_probs,\n",
    "                                           prev_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = viterbivars\n",
    "for k,v in tag_to_ix.items():\n",
    "    print ('tag: ',k, ' score: ',scores[v].data.numpy(), ' back-pointer-tag: ',\n",
    "           ix_to_tag[bptrs[(all_tags).index(k)]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 3.4** Build the Viterbi trellis. (*0.5 points*)\n",
    "\n",
    "This function should compute the `best_path` and the `path_score`. This function takes in the `emission_scores` for each particular token in the sentence, the `tag_transitions_weight` and returns the best set of tags for that particular sequence. Use `viterbi_step` to implement `build_trellis` in `viterbi.py` in Pytorch. \n",
    "\n",
    "This function should take:\n",
    "- **Inputs**:\n",
    "    - `all_tags`: a list of all tags: includes START_TAG and END_TAG\n",
    "    - `tag_to_ix`: a dictionary that maps each tag to a unique id.\n",
    "    - `cur_tag_scores`: a list of pytorch Variables where each contains the local emission score for each tag for that particular token in the sentence, len(cur_tag_scores) will be equal to len(words): each pytorch variables size would be equal to len(all_tags) indicating the score for each_tag.\n",
    "    - `transition_scores`: pytorch Variable (a matrix) that contains the tag_transition_scores\n",
    "\n",
    "- **Outputs**:\n",
    "    - `path_score`: the score for the best_path\n",
    "    - `best_path`: the actual best_path, which is the list of tags for each token: exclude the `START_TAG` and `END_TAG` here.   \n",
    "    \n",
    "- **Tests**: ```test_viterbi.py: test_trellis_score(), test_build_trellis()```\n",
    "First, make sure to pass the ```test_trellis_score()``` test and then move on to the ```test_build_trellis()``` test.\n",
    "\n",
    "** Note that for the input cur_tag_scores: we are sending in a list of pytorch variables: one for each token in the sentence to be tagged **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure START_TAG and END_TAG are in all_tags\n",
    "print (all_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- consider the same sentence as above: 'they can can fish'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = 'they can can fish'.split()\n",
    "print (words)\n",
    "print (word_to_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Below, we create `cur_tag_scores` using the `emission_probs` for each word in the sentence `'they can can fish'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing cur_tag_scores for the above sentence 'they can can fish'\n",
    "cur_tag_scores=[emission_probs[0],emission_probs[1],emission_probs[1],emission_probs[2]];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Complete the code to obtain the correct path_score initially and then use the backpointers to obtain the best_path. \n",
    "- Observe the inputs we are sending in for our example: 'they can can fish'\n",
    "    - `all_tags`: list of all tags including the `START_TAG` and `END_TAG`\n",
    "    - `tag_to_ix`: a mapping from tags to their unique ids\n",
    "    - `cur_tag_scores`: a list of pytorch variables: where each one is the score for each tag for a particular token. We send in these scores for each token in the sentence.\n",
    "    - `tag_transition_probs`: tag transition probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(viterbi);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_score, best_path = viterbi.build_trellis(all_tags, \n",
    "                                                  tag_to_ix, \n",
    "                                                  cur_tag_scores, \n",
    "                                                  tag_transition_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (best_path)\n",
    "print (path_score.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = ['they','can','can','can','can','fish']\n",
    "cur_tag_scores = [emission_probs[word_to_ix[w]] for w in sentence]\n",
    "\n",
    "path_score, best_path = viterbi.build_trellis(all_tags, \n",
    "                                                  tag_to_ix, \n",
    "                                                  cur_tag_scores, \n",
    "                                                  tag_transition_probs)\n",
    "print (best_path, path_score.data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Hidden Markov Model: Estimation\n",
    "\n",
    "You will now implement the estimatation for a hidden Markov model.\n",
    "\n",
    "We'll start with the tag transitions.\n",
    "\n",
    "**Deliverable 4.1** (*0.5 points* for 4650, *0.25 points* for 7650) Complete the function `most_common.get_tag_trans_counts()`.  This function should get the tag transition counts from the each tag to all possible tags. Don't forget to add the transitions from the `START_TAG` and the transitions from the `END_TAG`.\n",
    "\n",
    "You should use the `preproc.conll_seq_generator()` function.  \n",
    "\n",
    "- **Inputs**: `trainfile`, name of file containing training data\n",
    "- **Outputs**: a dictionary where keys are current tags and values are counters of succeeding tags.\n",
    "- **Tests**: ```test_hmm_trans_counts.py: test_tag_trans_counts()```  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(most_common);\n",
    "tag_trans_counts = most_common.get_tag_trans_counts(TRAIN_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function returns a dict of counters, where the keys are tags.\n",
    "\n",
    "Each counter is the frequency of tags following a given tag, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (tag_trans_counts['DET'])\n",
    "print (tag_trans_counts[START_TAG])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 4.2** Estimate transition log-probabilities for an HMM. (*0.5 points* for 4650, *0.25 points* for 7650)\n",
    "\n",
    "Implement `compute_transition_weights` in `hmm.py`. This function should return a dictionary of weights such that ```weights[(tag2,tag1)]``` = indicates the weights for transitions from `tag1` $\\rightarrow$ `tag2`. These weights will be used later for the Viterbi Tagger.\n",
    "\n",
    "### Inputs\n",
    "- Transition counts (generated from `get_tag_trans_counts`)\n",
    "- Smoothing\n",
    "\n",
    "### Outputs\n",
    "- Defaultdict with weights for transition features, in the form $[(y_m,y_{m-1})]$\n",
    "\n",
    "### Tests\n",
    "```test_hmm_trans.py: test_hmm_trans_weights_sum_to_one(), test_hmm_trans_weights_exact_vals() ```  \n",
    "\n",
    "Hints: \n",
    "\n",
    "- Don't forget to assign smoothed probabilities to transitions which do not appear in the counts. \n",
    "- Do not assign probabilities for transitions to the `START_TAG`, which can only come first. This will also affect your computation of the denominator, since you are not smoothing the probability of transitions to the `START_TAG`.\n",
    "- Don't forget to assign probabilities to transitions to the `END_TAG`; this too will affect your denominator.\n",
    "- As always, probabilities should sum to one (this time conditioned on the previous tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(hmm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm_trans_weights = hmm.compute_transition_weights(tag_trans_counts,.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (tag_trans_counts[START_TAG]['NOUN'], hmm_trans_weights[('NOUN',START_TAG)])\n",
    "print (tag_trans_counts[START_TAG]['VERB'], hmm_trans_weights[('VERB',START_TAG)])\n",
    "print (tag_trans_counts['DET']['VERB'], hmm_trans_weights[('VERB','DET')])\n",
    "print (tag_trans_counts['DET']['INTJ'], hmm_trans_weights[('INTJ','DET')])\n",
    "print (tag_trans_counts['DET']['NOUN'], hmm_trans_weights[('NOUN','DET')])\n",
    "print (tag_trans_counts['VERB'][START_TAG], hmm_trans_weights[(START_TAG,'VERB')])\n",
    "#print (tag_trans_counts[END_TAG]['VERB']) # will throw key error\n",
    "print (hmm_trans_weights[('VERB',END_TAG)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These log-probabilities should normalize to when summing over $y_m$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating all tags here, we also add END_TAG here.\n",
    "all_tags = sorted(list(tag_trans_counts.keys()) + [END_TAG])\n",
    "print (sum(np.exp(hmm_trans_weights[(tag,'NOUN')]) for tag in all_tags))\n",
    "print (sum(np.exp(hmm_trans_weights[(tag,'SYM')]) for tag in all_tags))\n",
    "print (sum(np.exp(hmm_trans_weights[(tag,'ADJ')]) for tag in all_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Now let us compute the weight variables for the whole dataset**\n",
    "- So, we recalculate them below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recalculating nb_weights for the whole dataset\n",
    "nb_weights = naive_bayes.get_nb_weights(TRAIN_FILE, .01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recalculating tag_to_ix={}\n",
    "tag_to_ix={}\n",
    "for tag in list(all_tags):\n",
    "    if tag not in tag_to_ix:\n",
    "        tag_to_ix[tag]=len(tag_to_ix)\n",
    "print (tag_to_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Note about OOV's**: We provide a helper function to calculate the `vocab` as shown below. We add an `UNK` token to the `vocab`. This is useful because, when we don't find a token's emission weight, we choose the `UNK` tokens weight and proceed with our tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recalculating vocab for the whole dataset now. # we also add an UNK token to the vocab here\n",
    "reload(most_common);\n",
    "vocab, word_to_ix = most_common.get_word_to_ix(TRAIN_FILE) #obtains all the words in the file\n",
    "print (len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emission_probs, tag_transition_probs = hmm.compute_weights_variables(nb_weights, hmm_trans_weights, \n",
    "                                                                     vocab, word_to_ix, tag_to_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 4.3** (*0.5 points*)\n",
    "\n",
    "We can now combine `Viterbi` and the `HMM` weights to compute the tag sequence for the example sentence. Make sure your implementation passes the test for this deliverable, and **explain (in `text-answers.md`) whether you think these predicted tags are correct**, based on your understanding of the universal part-of-speech tag set.\n",
    "\n",
    "- **Tests**: ```test_hmm.py: test_hmm_on_example_sentence()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure all_tags has END_TAG\n",
    "print (all_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(viterbi);\n",
    "viterbi.build_trellis(all_tags,\n",
    "                      tag_to_ix,\n",
    "                      [emission_probs[word_to_ix[w]] for w in ['they', 'can', 'can', 'fish','.']], \n",
    "                      tag_transition_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 4.4** (*0.5 points*)\n",
    "\n",
    "- Run your HMM tagger on the dev data and test data, using the code blocks below.\n",
    "- **Tests**: ```test_hmm.py: test_hmm_dev_accuracy(), test_hmm_test_accuracy()```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Observe that, based on our definition of the viterbi function, we need to send in two sets of important scores to the `build_trellis()` function\n",
    "    - cur_tag_scores: a list of emission scores for each tag for each token in the sentence\n",
    "    - tag_transition_probs: tag transition scores\n",
    "- When using the `HMM` with `Viterbi` Tagger: we have calculated the `cur_tag_scores` using the `naive_bayes_weights` and `tag_transition_probs` in 4.1.\n",
    "- As I have already mentioned above, for `cur_tag_scores`, we are sending in a list of pytorch variables: one for each token in the sentence to be tagged\n",
    "- Below, in the tagger that we create: we first calculate the set of `cur_tag_scores` for the words in the sentence and then send them in. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(viterbi);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Observe the way `cur_tag_scores` is computed in the loop below: \n",
    "    - For each particular word in a sentence: we assign the respective emission scores if it is present in our `vocab`, else we assign the emission_scores of an `UNK` token.\n",
    "    - This is repeated everywhere from now on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is just for fun\n",
    "for i,(words,_) in enumerate(preproc.conll_seq_generator(DEV_FILE)):\n",
    "    cur_tag_scores = [emission_probs[word_to_ix[w]] \n",
    "                      if w in word_to_ix else emission_probs[word_to_ix[UNK]] for w in words]\n",
    "    \n",
    "    pred_tags = viterbi.build_trellis(all_tags,\n",
    "                                      tag_to_ix,\n",
    "                                      cur_tag_scores,\n",
    "                                      tag_transition_probs)[1]\n",
    "    for word,pred_tag in zip(words,pred_tags):\n",
    "        print (\"%s/%s\"%(word,pred_tag),end=\" \")\n",
    "    print ('\\n')\n",
    "    if i >= 2: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = lambda words, all_tags : viterbi.build_trellis(all_tags, \n",
    "                                                        tag_to_ix,\n",
    "                                                            [emission_probs[word_to_ix[w]] \n",
    "                                                             if w in word_to_ix \n",
    "                                                             else emission_probs[word_to_ix[UNK]] \n",
    "                                                             for w in words],\n",
    "                                                            tag_transition_probs)[1]\n",
    "confusion = tagger_base.eval_tagger(tagger,'hmm-dev-en.preds', all_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (scorer.accuracy(confusion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger_base.apply_tagger(tagger,'hmm-te-en.preds',all_tags, testfile=TEST_FILE_HIDDEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you don't have en-ud-test.conllu, so you can't run this\n",
    "te_confusion = scorer.get_confusion(TEST_FILE,'hmm-te-en.preds')\n",
    "print (scorer.accuracy(te_confusion))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-Speech Tagging in Norwegian\n",
    "**Deliverable 4.5** (*0.5 points*)\n",
    "- Now, let us do part of speech tagging for data in Norwegian language using the Viterbi Tagger.\n",
    "- **Tests**: ```test_hmm.py: test_nr_hmm_dev_accuracy(), test_nr_hmm_test_accuracy()```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First, we calculate the `nb_weights`/emission weights for the norwegian language in a similar way as we did for the english language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recalculating nb_weights for the whole dataset\n",
    "nb_weights_nr = naive_bayes.get_nb_weights(NR_TRAIN_FILE, .01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now, we calculate the `tag_transition_weights` for the norwegian language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_trans_counts_nr = most_common.get_tag_trans_counts(NR_TRAIN_FILE)\n",
    "hmm_trans_weights_nr = hmm.compute_transition_weights(tag_trans_counts_nr,.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now, we obtain the `vocab`, `word_to_ix` and `tag_to_ix` below for the norwegian language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using helper functions to obtain vocab, word_to_ix, tag_to_ix\n",
    "all_tags_nr = sorted(list(tag_trans_counts_nr.keys()) + [END_TAG])\n",
    "vocab_nr, word_to_ix_nr = most_common.get_word_to_ix(NR_TRAIN_FILE) #obtains all the words in the vocab\n",
    "tag_to_ix_nr={}\n",
    "for tag in list(all_tags_nr):\n",
    "    tag_to_ix_nr[tag]=len(tag_to_ix_nr)\n",
    "print (tag_to_ix_nr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now, we convert these weights into pytorch variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emission_probs_nr, tag_transition_probs_nr = hmm.compute_weights_variables(nb_weights_nr, hmm_trans_weights_nr, \n",
    "                                                                           vocab_nr, word_to_ix_nr, tag_to_ix_nr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now, we construct a viterbi tagger for the norwegian language using these weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = lambda words, all_tags : viterbi.build_trellis(all_tags_nr, \n",
    "                                                           tag_to_ix_nr,\n",
    "                                                            [emission_probs_nr[word_to_ix_nr[w]] \n",
    "                                                             if w in word_to_ix_nr \n",
    "                                                             else emission_probs_nr[word_to_ix_nr[UNK]] \n",
    "                                                             for w in words],\n",
    "                                                            tag_transition_probs_nr)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = tagger_base.eval_tagger(tagger,'hmm-dev-nr.preds', all_tags_nr,\n",
    "                                    trainfile=NR_TRAIN_FILE,\n",
    "                                    testfile=NR_DEV_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (scorer.accuracy(confusion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger_base.apply_tagger(tagger,'hmm-te-nr.preds',all_tags_nr, \n",
    "                         trainfile=NR_TRAIN_FILE, testfile=NR_TEST_FILE_HIDDEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you don't have no_bokmaal-ud-test.conllu, so you can't run this\n",
    "te_confusion = scorer.get_confusion(NR_TEST_FILE,'hmm-te-nr.preds')\n",
    "print (scorer.accuracy(te_confusion))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. BiLSTM model for Part of Speech Tagging\n",
    "=========================\n",
    "\n",
    "A `BiLSTM` model for part-of-speech tagging takes as input the word embeddings of the tokens in the sentence, and passes them through an `LSTM`. For each token, the hidden state is used as input to a network that computes a score for each tag. A softmax layer then converts these scores to probabilities. This model should be trained end-to-end with the cross-entropy loss function.\n",
    "\n",
    "We will be building this `BiLSTM` model as a class using pytorch. Your implementation will include three functions:\n",
    "\n",
    "- `BiLSTM.__init__()`: define all the necessary model parameters\n",
    "    1. The word-embedding matrix, which maps the words to vectors\n",
    "    2. A BiLSTM Neural Network, which takes the word embeddings for the words as inputs and produces a hidden state for each token.\n",
    "    3. A one layer feedforward Neural Network, which projects the hidden state to a vector of scores for each tag\n",
    "- `forward()`: pass the input through the model, obtaining probability distributions over tags\n",
    "    1. Convert all the words to their word-vectors from the word-embedding matrix\n",
    "    2. Pass these word-vectors through a BiLSTM to obtain hidden states for the tokens\n",
    "    3. Pass these hidden states through the feedforward neural network to obtain the probability distributions of tags for each token.\n",
    "- `cross_entopy_loss()`, the training objective\n",
    "\n",
    "The description below provides additional help for each of these functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recalculating vocab: obtains the most common 6900 words from the file\n",
    "vocab, word_to_ix = most_common.get_word_to_ix(TRAIN_FILE, 6900)\n",
    "print ('words in the vocabulary: ',len(word_to_ix))\n",
    "print (word_to_ix[UNK])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- updating `tag_to_ix` and `all_tags` to remove `START_TAG` and `END_TAG`: these labels are not necessary in tagging with `BiLSTM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if START_TAG in all_tags:\n",
    "    all_tags.remove(START_TAG)\n",
    "if END_TAG in all_tags:\n",
    "    all_tags.remove(END_TAG)\n",
    "tag_to_ix={}\n",
    "for tag in all_tags:\n",
    "    if tag not in tag_to_ix:\n",
    "        tag_to_ix[tag] = len(tag_to_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Take a look at the helper functions `preproc.load_data(...)` and `bilstm.prepare_sequence(...)` \n",
    "    - `preproc.load_data(...)`: loads the data into a list of lists\n",
    "    - `bilstm.prepare_sequence(...)` given a sequence of words/tags and the `to_ix` dictionary that maps them to its unique indices: it returns a sequence of its unique indices.\n",
    "- The function `prepare_sequence()` will be used a lot from now on to convert the input to a `torch.LongTensor` and then send it to the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Loading Train data for english:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(preproc);\n",
    "X_tr, Y_tr = preproc.load_data(TRAIN_FILE)\n",
    "\n",
    "print (bilstm.prepare_sequence(X_tr[5],word_to_ix).data.numpy())\n",
    "print (bilstm.prepare_sequence(Y_tr[5],tag_to_ix).data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Loading Dev data for english:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dv, Y_dv = preproc.load_data(DEV_FILE)\n",
    "# loading dev data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Loading Test data for english:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_te, Y_te = preproc.load_data(TEST_FILE_HIDDEN)\n",
    "# loading test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 5.1**: (*0.5 points*)\n",
    "- Complete the `__init__()` function in the Class `bilstm.BiLSTM` that defines the model parameters:\n",
    "    - **Inputs**:\n",
    "        - `vocab_size`: vocab size of the model\n",
    "        - `tag_to_ix`: tag_to_ix: a dictionary that maps the tags to its unique id\n",
    "        - `embedding_dim`: embedding dimension for the words\n",
    "        - `hidden_dim`: hidden dimension for the `Bi-LSTM` model\n",
    "        - `embeddings`: embedding matrix of size: vocab_size x embedding_dim (this is to initialize embeddings with pretrained embeddings).\n",
    "    - The function does the following:\n",
    "        - Create an embedding matrix using torch.nn.Embedding of the size vocab \n",
    "            - [check this pytorch_doc_for_embedding](http://pytorch.org/docs/0.3.0/nn.html?highlight=embedding#torch.nn.Embedding)\n",
    "        - Create a Bi-LSTM model with just one layer, and hidden dimension = hidden_dim \n",
    "            - [check this pytorch_doc_for_LSTM](http://pytorch.org/docs/0.3.0/nn.html?highlight=lstm#torch.nn.LSTM)\n",
    "        - Also add a FullyConnected Layer, that would project the hidden state onto the tag space. \n",
    "            - [check this pytorch_doc_for_Linear](http://pytorch.org/docs/0.3.0/nn.html?highlight=linear#torch.nn.Linear)\n",
    "    - Make sure to name the parameters as follows: The unit tests will check for these variables\n",
    "        - `self.word_embeds`\n",
    "        - `self.lstm`\n",
    "        - `self.hidden2tag`\n",
    "    - **Tests**: ```test_bilstm.py: test_dlmodel_init()```\n",
    "    - All you need to do here is to define the model parameters here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Below, you can find brief example of how these components are created in Torch. In these examples, the dimensions are arbitrary; you will need to determine the correct dimensions for your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this basically defines a matrix of embeddings where the vocab_size=10 and the embedding_dim=10 \n",
    "word_embeds = nn.Embedding(num_embeddings=10, embedding_dim=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following line is how you define an LSTM such that the input_size of the vector=10, \n",
    "# hidden state of each LSTM (forward and backward) =20, num of layers=1, \n",
    "# bidirectional=True indicating both forward and backward LSTM will be included.\n",
    "lstm = nn.LSTM(input_size=10, hidden_size = 20, num_layers=1, bidirectional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following line is the way to define a Fully Connected Layer with input_dim=40, output_dim=10 and bias=True.\n",
    "hidden2tag = nn.Linear(in_features=40, out_features=10, bias=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Once, you have defined the parameters of the model: check if you have done it right using the unit test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(bilstm);\n",
    "torch.manual_seed(765);\n",
    "embedding_dim=30\n",
    "hidden_dim=30\n",
    "model = bilstm.BiLSTM(len(word_to_ix),tag_to_ix,embedding_dim, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 5.2** (*0.5 points*)\n",
    "- Complete the `bilstm.BiLSTM.forward()` function in the `bilstm.BiLSTM` class to obtain the scores for each tag for each of the words in a sentence\n",
    "- **Input**:\n",
    "    - sentence: a sequence of ids for each word in the sentence\n",
    "- The function does the following:\n",
    "    - Obtains the embeddings for the input sequence\n",
    "    - passes them through an `LSTM` to get the respective hidden states; use the hidden state initialized in the function.\n",
    "    - projects them onto the tag-space using the FC layer\n",
    "- Make sure to reshape the embeddings of the words before sending them to the `BiLSTM`. The axes semantics are: `seq_len, mini_batch, embedding_dim`.\n",
    "- You can use the .view() method to reshape a tensor. You might need to use this, as the neural network components expect their inputs to have a certain shape. [check the pytorch doc on view](http://pytorch.org/docs/0.3.0/tensors.html?highlight=view#torch.Tensor.view)\n",
    "- **Tests**: ```test_bilstm.py: test_dlmodel_forward()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the random seed\n",
    "reload(bilstm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating your model\n",
    "torch.manual_seed(765);\n",
    "model = bilstm.BiLSTM(len(word_to_ix),tag_to_ix,embedding_dim, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing your first sentence to be an input\n",
    "words= X_tr[5]\n",
    "tags = Y_tr[5]\n",
    "sentence = bilstm.prepare_sequence(words, word_to_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- this calls the `forward()` function on the model, which returns the tag_scores for each tag for each particular token in the sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_feats = model(sentence)\n",
    "print (lstm_feats[0][0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we provide the `predict()` function that returns the set of tags obtained for the specific input by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = model.predict(sentence)\n",
    "print (tags[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train the model for now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(bilstm);\n",
    "torch.manual_seed(765);\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "model, losses, accuracies = bilstm.train_model(loss, model, X_tr,Y_tr, word_to_ix, tag_to_ix, \n",
    "                                        X_dv, Y_dv, num_its=10, status_frequency=2, \n",
    "                                        optim_args = {'lr':0.1,'momentum':0}, param_file = 'best.params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(bilstm);\n",
    "bilstm.plot_results(losses, accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 5.3**: (*0.5 points*)\n",
    "- As you can see from above, BiLSTM model performs worse than the Viterbi tagger.\n",
    "- Use pretrained embeddings like the following to improve your performance.\n",
    "- Check the following sources: [fastText](https://github.com/facebookresearch/fastText), [Polyglot](polyglot.readthedocs.io/en/latest/Embeddings.html), [word2vec](https://code.google.com/archive/p/word2vec/), [Glove](https://nlp.stanford.edu/projects/glove/)\n",
    "- Tune the hyperparameters such that you obtain atleast `85.00%` accuracy on the dev set.\n",
    "- You can try changing the no. of iterations, optimizers, no. of LSTM layers, the hidden dimension units, the FC layer dimensions, or use pretrained word embeddings like word2vec, fastText, polyglot, character features etc..\n",
    "- **Tests**: ```test_bilstm.py: test_bilstm_dev_accuracy(), test_bilstm_test_accuracy()```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Remember: After training a model once, everytime when you rerun the train_model(...) function, the model is retrained on top of the previous parameters. If you want to start afresh, make sure the model is reinitialized and then trained.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: pretrained embeddings\n",
    "\n",
    "- *Polyglot* provides pre-trained word embeddings for many languages\n",
    "- You can download the english polyglot embeddings [here](https://sites.google.com/site/rmyeid/projects/polyglot).\n",
    "- Use the following helper function to load in the polyglot embeddings, which you can use to initialize your parameter embeddings in your model. You need to make sure your embedding_dim matches your embeddings size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(bilstm);\n",
    "filename = 'data/polyglot-en.pkl'\n",
    "word_embeddings = bilstm.obtain_polyglot_embeddings(filename, word_to_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let us have a look at these word embeddings. We can observe the cosine similarity between two word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(emb1, emb2): #function to return the cosine similarity between the embeddings\n",
    "    return emb1.dot(emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "diremb = word_embeddings[word_to_ix['dire']]\n",
    "catemb = word_embeddings[word_to_ix['catastrophic']]\n",
    "amazemb = word_embeddings[word_to_ix['success']]\n",
    "print(cosine(diremb,catemb)) # dire and catastrophic are similar\n",
    "print(cosine(diremb,amazemb))\n",
    "print(cosine(catemb,amazemb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now, all we need to do is send in the word_embeddings when initializing your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(765);\n",
    "embedding_dim=64\n",
    "hidden_dim=30\n",
    "model = bilstm.BiLSTM(len(word_to_ix),tag_to_ix,embedding_dim, hidden_dim, word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.CrossEntropyLoss()\n",
    "model, losses, accuracies = bilstm.train_model(loss, model, X_tr,Y_tr, word_to_ix, tag_to_ix, \n",
    "                                        X_dv, Y_dv, num_its=5, status_frequency=1, \n",
    "                                        optim_args = {'lr':0.01,'momentum':0}, param_file = 'best.params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(bilstm);\n",
    "bilstm.plot_results(losses, accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(tagger_base);\n",
    "reload(bilstm);\n",
    "confusion = tagger_base.eval_model(model,'bilstm-dev-en.preds', word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (scorer.accuracy(confusion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger_base.apply_model(model,'bilstm-te-en.preds',word_to_ix, all_tags, testfile=TEST_FILE_HIDDEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you don't have en-ud-test.conllu, so you can't run this\n",
    "te_confusion = scorer.get_confusion(TEST_FILE,'bilstm-te-en.preds')\n",
    "print (scorer.accuracy(te_confusion))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You may want to try other ideas like: learning rate, hidden layer sizes, optimizers, other pretrained embeddings for english etc. to improve the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of Speech Tagging for Norwegian\n",
    "Make sure your code runs on the Norwegian dataset as well. You may want to use the pretrained embeddings for norwegian to improve the performance of the model. However, there is no deliverable or test for this part.\n",
    "- Run the `BiLSTM` model on the norwegian dataset using the `BiLSTM` tagger. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recalculating all_tags and tag_to_ix for the norwegian language\n",
    "if START_TAG in all_tags_nr:\n",
    "    all_tags_nr.remove(START_TAG)\n",
    "if END_TAG in all_tags_nr:\n",
    "    all_tags_nr.remove(END_TAG)\n",
    "\n",
    "tag_to_ix_nr={}\n",
    "for tag in all_tags_nr:\n",
    "    if tag not in tag_to_ix_nr:\n",
    "        tag_to_ix_nr[tag] = len(tag_to_ix_nr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recalculating the vocab for the norwegian language: obtains the most common 7600 words from the file\n",
    "vocab_nr, word_to_ix_nr = most_common.get_word_to_ix(NR_TRAIN_FILE, 7600)\n",
    "print ('words in the vocabulary: ',len(word_to_ix_nr))\n",
    "print (word_to_ix_nr[UNK])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Loading training data for norwegian:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(preproc);\n",
    "X_tr_nr, Y_tr_nr = preproc.load_data(NR_TRAIN_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Loading dev data for norwegian:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dv_nr, Y_dv_nr = preproc.load_data(NR_DEV_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Loading test data for norwegian:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_te_nr, Y_te_nr = preproc.load_data(NR_TEST_FILE_HIDDEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up the random seed\n",
    "torch.manual_seed(765);\n",
    "\n",
    "# initializing your model\n",
    "embedding_dim=30\n",
    "hidden_dim=30\n",
    "model = bilstm.BiLSTM(len(word_to_ix_nr),tag_to_ix_nr,embedding_dim, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training your model for norwegian data\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "model, losses, accuracies = bilstm.train_model(loss, model, X_tr_nr,Y_tr_nr, word_to_ix_nr, tag_to_ix_nr, \n",
    "                                        X_dv_nr, Y_dv_nr, num_its=10, status_frequency=2, \n",
    "                                        optim_args = {'lr':0.2,'momentum':0}, param_file = 'best.params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(bilstm);\n",
    "bilstm.plot_results(losses, accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(tagger_base);\n",
    "reload(bilstm);\n",
    "confusion = tagger_base.eval_model(model,'bilstm-dev-nr.preds', word_to_ix_nr, \n",
    "                                   trainfile=NR_TRAIN_FILE, testfile=NR_DEV_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (scorer.accuracy(confusion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger_base.apply_model(model,'bilstm-te-nr.preds',word_to_ix_nr, all_tags_nr, \n",
    "                        trainfile=NR_TRAIN_FILE, testfile=NR_TEST_FILE_HIDDEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you don't have no_bokmaal-ud-test.conllu, so you can't run this\n",
    "te_confusion = scorer.get_confusion(NR_TEST_FILE,'bilstm-te-nr.preds')\n",
    "print (scorer.accuracy(te_confusion))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Using cuda\n",
    "If you want to run the code on a GPU: \n",
    "- make sure you convert the input and target Tensors to be cuda Variables like below:\n",
    "\n",
    "`inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())`\n",
    "- and convert your model to cuda:\n",
    "\n",
    " `model.cuda()`\n",
    "\n",
    "The following links would be useful: when you want to run your model on a GPU:  \n",
    " - [pytorch_doc](http://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#training-on-gpu)\n",
    " - [mnist pytorch gpu example](https://github.com/pytorch/examples/blob/master/mnist/main.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Adding a CRF on top of BiLSTM\n",
    "=================\n",
    "\n",
    "We will now be building a `CRF` on top of the `BiLSTM` for Part-of-Speech tagging. The LSTM tagger above is typically sufficient for part-of-speech tagging, but a sequence model like the CRF might be helpful for tasks like Named Entity Recognition where the transitions and the overall structure is very important.\n",
    "\n",
    "Recall that the `CRF` computes a conditional probability. Let Y be a tag sequence and X an input sequence of words. Then we compute  \n",
    "\n",
    "$P (Y \\mid X) = \\frac{exp(Score(X,Y))}{\\sum_{Y_i}exp(Score(X,Y_i))}$    \n",
    "where the the score is determined by defining some log potentials $\\log\\Psi(X,Y)$ such that these potential only look at the local features.  \n",
    "\n",
    "In the Bi-LSTM CRF, we define two kinds of potentials: emission and transition. \n",
    "- The emission potential for the word at index $i$ comes from the hidden state of the Bi-LSTM at timestep $i$. \n",
    "- The transition scores are stored in a `|T|x|T|` matrix `transitions`, where T is the tag set. In my implementation, `transitions[j][k]` is the score of transitioning to tag k from tag j.\n",
    "\n",
    "$ Score(X,Y) = \\sum_{i}\\log\\Psi_{emit}(y_i \\rightarrow x_i) + \\sum_{i}\\log\\Psi_{trans}(y_{i-1} \\rightarrow y_{i}) \\\\\n",
    " Score(X,Y) = \\sum_{i} h_{i}[y_{i}] + transitions_{y_i,y_{i-1}} $\n",
    " \n",
    "See the notes for more details on this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Since, we are now using a `CRF`, we need to add `START_TAG` and `END_TAG` to the tagset: so, we modify `tag_to_ix` and `ix_to_tag` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding START_TAG and END_TAG to all_tags, updating tag_to_ix\n",
    "if START_TAG not in all_tags:\n",
    "    all_tags.append(START_TAG)\n",
    "if END_TAG not in all_tags:\n",
    "    all_tags.append(END_TAG)\n",
    "all_tags = sorted(all_tags)\n",
    "if START_TAG not in tag_to_ix:\n",
    "    tag_to_ix[START_TAG] = len(tag_to_ix)\n",
    "if END_TAG not in tag_to_ix:\n",
    "    tag_to_ix[END_TAG] = len(tag_to_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have already provided the transitions parameter to the `bilstm.BiLSTM_CRF` class. Take a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting random seed\n",
    "reload(bilstm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing model\n",
    "torch.manual_seed(765);\n",
    "embedding_dim=30\n",
    "hidden_dim=30\n",
    "model = bilstm.BiLSTM_CRF(len(word_to_ix),tag_to_ix,embedding_dim, hidden_dim)\n",
    "print (model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Observe that `BiLSTM_CRF` class is derived from the `BiLSTM` class\n",
    "- Note that we would be using the forward function from `bilstm.BiLSTM` class to obtain the lstm hidden states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us prepare an input sequence\n",
    "sentence = bilstm.prepare_sequence(X_tr[5], word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_feats = model.forward(sentence)\n",
    "print (lstm_feats[0][0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 6.1** (*0.5 points*) \n",
    "Complete the `bilstm.BiLSTM_CRF.forward_alg()` function to do the following:  \n",
    "This function calculates the log likelihood score for a particular sentence. It works very similar to the `viterbi` algorithm: Instead of finding the maximum `prev_tag`, you need to calculate the probability to arrive at the `curr_tag` for the `curr_token`. The forward algorithm is described in chapter 6 of the class notes.\n",
    "\n",
    "- **Inputs**: `feats`, the hidden states for each token in the input_sequence. Consider this to be the emission potential of each token for each tag.\n",
    "    - Make sure to use the `self.transitions` that is defined to capture the tag-transition probabilities\n",
    "- **Outputs**: `alpha`, a pytorch variable containing the scalar score for the entire input, $\\log \\sum_{y_{1:M}} P(w, y)$ \n",
    "- **Tests**: ```test_bilstm_crf.py: test_forward_alg()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(765);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(bilstm);\n",
    "model = bilstm.BiLSTM_CRF(len(word_to_ix),tag_to_ix,embedding_dim, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passing an input sentence to obtain lstm_feats\n",
    "lstm_feats = model.forward(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = model.forward_alg(lstm_feats)\n",
    "print (alpha.data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 6.2** (*0.5 points*)\n",
    "Complete the function `bilstm.BiLSTM_CRF.score_sentence()` to obtain the joint log-likelihood score of the particular sequence of tokens and their tags.\n",
    "- **Inputs**:\n",
    "    - `feats`: the hidden state scores for each token in the input sentence. Consider this to be the emission potential of each token for each tag.\n",
    "    - `gold_tags`: a pytorch Variable of the gold sequence of tags: obtain the joint-log-likelihood score of the sequence with the feats and gold_tags.\n",
    "- **Outputs**:\n",
    "    - a pytorch variable of the score. \n",
    "- **Tests**: ```test_bilstm_crf.py: test_score_sentence()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(765);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(bilstm);\n",
    "model = bilstm.BiLSTM_CRF(len(word_to_ix),tag_to_ix,embedding_dim, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare both the tokens and tags to be passed as input\n",
    "sentence = bilstm.prepare_sequence(X_tr[5], word_to_ix)\n",
    "tags = bilstm.prepare_sequence(Y_tr[5], tag_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(765);\n",
    "lstm_feats = model.forward(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.score_sentence(lstm_feats, tags)\n",
    "print (score.data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Deliverable 6.3 ** (*0.5 points* for 4650, *0.25 points* for 7650) Complete the `bilstm.BiLSTM_CRF.predict()` function to decode the tags using the viterbi algorithm. Make sure to use the viterbi functions defined previously.\n",
    "- **Inputs**:\n",
    "    - `sentence`: a pytorch Variable of sequence of ids of the input_tokens\n",
    "- **Outputs**:\n",
    "    - `best_path`: a list of tags for the sequence of tokens\n",
    "- **Tests**: ```test_bilstm_crf.py: test_predict()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(bilstm);\n",
    "torch.manual_seed(765);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = bilstm.BiLSTM_CRF(len(word_to_ix),tag_to_ix,embedding_dim, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_tags = model.predict(sentence)\n",
    "print (best_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 6.4** (*0.5 points* for 4650, *0.25 points* for 7650)\n",
    "Complete the `bilstm.BiLSTM_CRF.neg_log_likelihood()` function to obtain the negative log likelihood loss. This function calculates the loss function for the `CRF`. Observe that, this can be easily calculated using the previously defined functions.  \n",
    "You should use the `forward(), forward_alg(), score_sentence()` functions defined previously.\n",
    "\n",
    "- **Tests**: ```test_bilstm_crf.py: test_neg_log_likelihood()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting random seed\n",
    "reload(bilstm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(765);\n",
    "model = bilstm.BiLSTM_CRF(len(word_to_ix),tag_to_ix,embedding_dim, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_feats = model.forward(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model.neg_log_likelihood(lstm_feats, bilstm.prepare_sequence(Y_tr[5], tag_to_ix))\n",
    "print (loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Putting all the above components together, let us train the `BiLSTM-CRF` model. **Don't worry about matching the scores for the following code blocks from now. Just Make sure that you pass the unit tests.** \n",
    "- You may want to use pretrained embeddings to improve the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(bilstm);\n",
    "torch.manual_seed(765);\n",
    "embedding_dim=30\n",
    "hidden_dim=30\n",
    "model = bilstm.BiLSTM_CRF(len(word_to_ix),tag_to_ix,embedding_dim, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sending loss function so that we can calculate the loss for the BiLSTM-CRF\n",
    "loss = model.neg_log_likelihood\n",
    "model, losses, accuracies = bilstm.train_model(loss, model, X_tr,Y_tr, word_to_ix, tag_to_ix,\n",
    "                                               X_dv, Y_dv, \n",
    "                                               num_its=10, status_frequency=2,\n",
    "                                               optim_args = {'lr':0.1,'momentum':0}, param_file = 'best.params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(bilstm);\n",
    "bilstm.plot_results(losses, accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Try and Tune the hyperparameters such that you improve the accuracy on the dev set.\n",
    "You can try changing the number of LSTM layers, the hidden dimension units, the FC layer dimensions, using pretrained embeddings etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(bilstm);\n",
    "torch.manual_seed(765);\n",
    "embedding_dim=30\n",
    "hidden_dim=30\n",
    "model = bilstm.BiLSTM_CRF(len(word_to_ix),tag_to_ix,embedding_dim, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sending loss function so that we can calculate the loss for the BiLSTM-CRF\n",
    "loss = model.neg_log_likelihood\n",
    "model, losses, accuracies = bilstm.train_model(loss, model, X_tr,Y_tr, word_to_ix, tag_to_ix,\n",
    "                                               X_dv, Y_dv, \n",
    "                                               num_its=5, status_frequency=1,\n",
    "                                               optim_args = {'lr':0.2,'momentum':0}, param_file = 'best.params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(bilstm);\n",
    "bilstm.plot_results(losses, accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(tagger_base);\n",
    "reload(bilstm);\n",
    "confusion = tagger_base.eval_model(model,'bilstm_crf-dev-en.preds', word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (scorer.accuracy(confusion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger_base.apply_model(model,'bilstm_crf-te-en.preds',word_to_ix, all_tags, testfile=TEST_FILE_HIDDEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you don't have en-ud-test.conllu, so you can't run this\n",
    "te_confusion = scorer.get_confusion(TEST_FILE,'bilstm_crf-te-en.preds')\n",
    "print (scorer.accuracy(te_confusion))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Part of Speech Tagging for Norwegian Language.\n",
    "We will use the same `BiLSTM-CRF` part of speech tagger for Norwegian now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Running a `BiLSTM-CRF` model on the norwegian language. Make sure your model runs on the norwegian data. \n",
    "- You may want to use the pretrained embeddings for norwegian to improve the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if START_TAG not in all_tags_nr:\n",
    "    all_tags_nr.append(START_TAG)\n",
    "if END_TAG not in all_tags_nr:\n",
    "    all_tags_nr.append(END_TAG)\n",
    "all_tags_nr = sorted(all_tags_nr)\n",
    "if START_TAG not in tag_to_ix_nr:\n",
    "    tag_to_ix_nr[START_TAG]=len(tag_to_ix_nr)\n",
    "if END_TAG not in tag_to_ix_nr:\n",
    "    tag_to_ix_nr[END_TAG]=len(tag_to_ix_nr)\n",
    "# we will be using this tag_to_ix and ix_to_tag from now on.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(bilstm);\n",
    "torch.manual_seed(765);\n",
    "embedding_dim=30\n",
    "hidden_dim=30\n",
    "model = bilstm.BiLSTM_CRF(len(word_to_ix_nr),tag_to_ix_nr,embedding_dim, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sending loss function so that we can calculate the loss for the BiLSTM-CRF\n",
    "loss = model.neg_log_likelihood\n",
    "model, losses, accuracies = bilstm.train_model(loss, model, X_tr_nr, Y_tr_nr, word_to_ix_nr, tag_to_ix_nr,\n",
    "                                               X_dv_nr, Y_dv_nr, \n",
    "                                               num_its=10, status_frequency=2,\n",
    "                                               optim_args = {'lr':0.1,'momentum':0}, param_file = 'best.params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(bilstm);\n",
    "bilstm.plot_results(losses, accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(tagger_base);\n",
    "reload(bilstm);\n",
    "confusion = tagger_base.eval_model(model,'bilstm_crf-dev-nr.preds', word_to_ix_nr, \n",
    "                                   trainfile=NR_TRAIN_FILE, testfile=NR_DEV_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (scorer.accuracy(confusion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger_base.apply_model(model,'bilstm_crf-te-nr.preds',word_to_ix_nr, all_tags_nr, \n",
    "                        trainfile=NR_TRAIN_FILE, testfile=NR_TEST_FILE_HIDDEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you don't have no_bokmaal-ud-test.conllu, so you can't run this\n",
    "te_confusion = scorer.get_confusion(NR_TEST_FILE,'bilstm_crf-te-nr.preds')\n",
    "print (scorer.accuracy(te_confusion))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Improve your Taggers performance\n",
    "==================\n",
    "**Deliverable 7.1** (*2.0 points*)\n",
    "Improve your taggers performance to get atleast `87.00%` on the dev dataset for both English and Norwegian.\n",
    "You can use any model that you have implemented in this assignment: HMM, BiLSTM, or BiLSTM-CRF.\n",
    "- HMM is unlikely to score high enough for full credit\n",
    "- BiLSTM-CRF is more accurate than BiLSTM in theory, but it takes a long time to train. So iterating on BiLSTM might be better.    \n",
    "- Ideas: try varying the input (character-level features?), the optimizer, the model architecture, the pretrained embeddings, dropout, regularization, etc. Here are some more pretrained embeddings to try: [fastText](https://github.com/facebookresearch/fastText), [Polyglot](polyglot.readthedocs.io/en/latest/Embeddings.html), [word2vec](https://code.google.com/archive/p/word2vec/), [Glove](https://nlp.stanford.edu/projects/glove/).\n",
    "- For Norwegian, there are officially [two written forms of language](https://en.wikipedia.org/wiki/Norwegian_language): `Bokml` and `Nynorsk`. The dataset we are using is for the `Bokml` language. So, when using pretrained embeddings make sure to use it for that specific form.\n",
    "\n",
    "**Output files:** Make sure to name your files as follows: The unit tests will check for these files.\n",
    "- `model-dev-en.preds` ( Predictions for the dev dataset of english ) \n",
    "- `model-te-en.preds`  ( Predictions for the test dataset of english ) \n",
    "- `model-dev-nr.preds` ( Predictions for the dev dataset of norwegian ) \n",
    "- `model-te-nr.preds`  ( Predictions for the test dataset of norwegian ) \n",
    "\n",
    "**Tests**: `test_performance.py`: \n",
    "- `test_en_dev_accuracy()`\n",
    "- `test_en_test_accuracy()` #you cannot run this unit test.\n",
    "- `test_nr_dev_accuracy()`\n",
    "- `test_nr_test_accuracy()` #you cannot run this unit test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rubric\n",
    "\n",
    "Dev Set (both languages)\n",
    "- $\\geq$ 85% (*0.15 points*)\n",
    "- $\\geq$ 86% (*0.25 points*)\n",
    "- $\\geq$ 87% (*0.5 points*)\n",
    "\n",
    "Test Set (both languages)\n",
    "- $\\geq$ 84% (*0.15 points*)\n",
    "- $\\geq$ 85% (*0.25 points*)\n",
    "- $\\geq$ 86% (*0.5 points*)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "8. Bakeoff (Extra Credit)\n",
    "=================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Try to get the best accuracy possible on both `English` and `Norwegian` tagging.\n",
    "\n",
    "Some ideas:\n",
    "\n",
    "- Better features, such as characters\n",
    "- Better optimization\n",
    "- Number of Layers and Hidden units in the BiLSTM\n",
    "- More hidden layers in the Fully Connected Layer\n",
    "- Better loss function: like structured perceptron\n",
    "- Better preprocessing\n",
    "- Using Pretrained Embeddings like [fastText](https://github.com/facebookresearch/fastText), [Polyglot](polyglot.readthedocs.io/en/latest/Embeddings.html), [word2vec](https://code.google.com/archive/p/word2vec/), [Glove](https://nlp.stanford.edu/projects/glove/)\n",
    "- Dropout or other regularization scheme\n",
    "\n",
    "The current best accuracies from the staff are:\n",
    "- For English data:    \n",
    "    - `88% dev`, \n",
    "    - `87.5% test`\n",
    "- For Norwegian data:  \n",
    "    - `89% dev`, \n",
    "    - `87.8% test`\n",
    "\n",
    "However, these accuracies may improve before the end of the bakeoff!   \n",
    " \n",
    "\n",
    "**Output files:** In addition to participating in the kaggle bakeoff, make sure to name your files as follows:\n",
    "\n",
    "- `bakeoff-dev-en.preds` ( Predictions for the dev dataset of english ) \n",
    "- `bakeoff-te-en.preds`  ( Predictions for the test dataset of english ) \n",
    "- `bakeoff-dev-nr.preds` ( Predictions for the dev dataset of norwegian ) \n",
    "- `bakeoff-te-nr.preds`  ( Predictions for the test dataset of norwegian ) \n",
    "\n",
    "The unit tests will check for these files. To get the extra credit, you must both participate in Kaggle and submit these files.\n",
    "\n",
    "**Rubric**\n",
    "\n",
    "- Top 3 final score on Kaggle in English: +0.5%\n",
    "- Top 3 final score on Kaggle in Norwegian: +0.5%\n",
    "- Beat best staff final score on Kaggle in English: +0.5%\n",
    "- Best best staff final score on Kaggle in Norwegian: +0.5%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 9. 7650 Research Question  \n",
    "\n",
    "(*1 point*)\n",
    "\n",
    "You will select a recent research paper that performs **sequence labeling** either POS tagging or some other task within the last five years (2014-2018). Summarize the paper, answering the following questions:\n",
    "\n",
    "- List the title, author(s) and venue of the paper.\n",
    "- What is the task they are trying to solve?\n",
    "- What tagging methods do they use? HMMs, CRF?, max-margin markov networks, deep learning models?\n",
    "- What features do they use and why?\n",
    "- What methods and features are most effective?\n",
    "- Give a one-line summary of the paper that the authors are trying to leave for the reader.\n",
    "\n",
    "Your selection of papers is determined by the last digit of your GTID.\n",
    "\n",
    "- Digits 0-4: choose from NAACL, EMNLP, or NIPS\n",
    "- Digits 5-9: choose from ACL, EACL, or TACL\n",
    "\n",
    "You must choose a paper in the main conference (not workshops). The paper must be at least four pages long. All papers from these conferences are available for free online."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
